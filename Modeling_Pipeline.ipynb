{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amulya-B28/EngageNet-Student-Modeling/blob/main/Modeling_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9lTrmKGtwhl",
        "outputId": "dc7bd672-0288-4986-bf8d-22f28e57a416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL COMBINATION COMPARISION"
      ],
      "metadata": {
        "id": "z-29b-SNFJml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Flatten, InputLayer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "info = pd.read_csv('/content/drive/MyDrive/Oulad/studentInfo.csv')\n",
        "assessment = pd.read_csv('/content/drive/MyDrive/Oulad/studentAssessment.csv')\n",
        "vle = pd.read_csv('/content/drive/MyDrive/Oulad/studentVle.csv')\n",
        "\n",
        "# Sample 5000 unique students\n",
        "unique_ids = vle['id_student'].drop_duplicates()\n",
        "sampled_ids = random.sample(list(unique_ids), 20000)\n",
        "vle = vle[vle['id_student'].isin(sampled_ids)]\n",
        "info = info[info['id_student'].isin(sampled_ids)]\n",
        "assessment = assessment[assessment['id_student'].isin(sampled_ids)]\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess(info, assessment, vle):\n",
        "    assessment_counts = assessment.groupby('id_student').size().reset_index(name='assessment_count')\n",
        "    avg_score = assessment.groupby('id_student')['score'].mean().reset_index(name='avg_score')\n",
        "    total_clicks = vle.groupby('id_student')['sum_click'].sum().reset_index(name='total_clicks')\n",
        "    active_days = vle.groupby('id_student')['date'].nunique().reset_index(name='active_days')\n",
        "\n",
        "    df = info.merge(assessment_counts, on='id_student', how='left')\n",
        "    df = df.merge(avg_score, on='id_student', how='left')\n",
        "    df = df.merge(total_clicks, on='id_student', how='left')\n",
        "    df = df.merge(active_days, on='id_student', how='left')\n",
        "\n",
        "    df[['assessment_count', 'avg_score', 'total_clicks', 'active_days']] = df[\n",
        "        ['assessment_count', 'avg_score', 'total_clicks', 'active_days']].fillna(0)\n",
        "\n",
        "    for col in ['gender', 'region', 'highest_education', 'imd_band', 'age_band', 'disability', 'final_result']:\n",
        "        df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
        "\n",
        "    df['binary_target'] = df['final_result'].apply(lambda x: 1 if x == 3 else 0)\n",
        "    return df\n",
        "\n",
        "df = preprocess(info, assessment, vle)\n",
        "\n",
        "# Features and target\n",
        "X = df[['gender', 'region', 'highest_education', 'imd_band', 'age_band',\n",
        "        'disability', 'assessment_count', 'avg_score', 'total_clicks', 'active_days']]\n",
        "y = df['binary_target']\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Class weights\n",
        "weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y)\n",
        "cw_dict = dict(enumerate(weights))\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "results = []\n",
        "\n",
        "# ----- Individual Models -----\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr_preds = lr.predict(X_test)\n",
        "lr_acc = accuracy_score(y_test, lr_preds)\n",
        "results.append(('LogReg', lr_acc))\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)\n",
        "rf_acc = accuracy_score(y_test, rf_preds)\n",
        "results.append(('RF', rf_acc))\n",
        "\n",
        "# LogReg + RF (average of predictions)\n",
        "combined_lr_rf_preds = np.round((lr_preds + rf_preds) / 2)\n",
        "combined_lr_rf_acc = accuracy_score(y_test, combined_lr_rf_preds)\n",
        "results.append(('LogReg + RF', combined_lr_rf_acc))\n",
        "\n",
        "# SVM + RF\n",
        "svm = SVC()\n",
        "svm.fit(X_train, y_train)\n",
        "svm_preds = svm.predict(X_test)\n",
        "svm_acc = accuracy_score(y_test, svm_preds)\n",
        "results.append(('SVM + RF', (svm_acc + rf_acc) / 2))\n",
        "\n",
        "# ANN + LSTM\n",
        "X_train_rnn = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
        "X_test_rnn = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    InputLayer(input_shape=(1, X_train.shape[1])),\n",
        "    LSTM(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(X_train_rnn, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "lstm_preds = (lstm_model.predict(X_test_rnn) > 0.5).astype('int32')\n",
        "lstm_acc = accuracy_score(y_test, lstm_preds)\n",
        "results.append(('ANN + LSTM', lstm_acc))\n",
        "\n",
        "# CNN + LSTM\n",
        "X_train_seq = np.expand_dims(X_train, axis=2)\n",
        "X_test_seq = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(32, 3, activation='relu', input_shape=(X_train_seq.shape[1], 1)),\n",
        "    MaxPooling1D(2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "cnn_model.fit(X_train_seq, y_train, epochs=3, batch_size=32, verbose=0)\n",
        "cnn_preds = (cnn_model.predict(X_test_seq) > 0.5).astype('int32')\n",
        "cnn_acc = accuracy_score(y_test, cnn_preds)\n",
        "results.append(('CNN + LSTM', cnn_acc))\n",
        "\n",
        "# XGBoost + LightGBM\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb_preds = xgb.predict(X_test)\n",
        "xgb_acc = accuracy_score(y_test, xgb_preds)\n",
        "\n",
        "lgbm = LGBMClassifier()\n",
        "lgbm.fit(X_train, y_train)\n",
        "lgbm_preds = lgbm.predict(X_test)\n",
        "lgbm_acc = accuracy_score(y_test, lgbm_preds)\n",
        "\n",
        "combined_xgb_lgbm_preds = np.round((xgb_preds + lgbm_preds) / 2)\n",
        "combined_xgb_lgbm_acc = accuracy_score(y_test, combined_xgb_lgbm_preds)\n",
        "results.append(('XGB + LGBM', combined_xgb_lgbm_acc))\n",
        "\n",
        "# ----- Final Results -----\n",
        "print(\"\\nFinal Model Accuracy Comparison (Binary Classification):\")\n",
        "for model, acc in results:\n",
        "    print(f\"{model}: {acc * 100:.2f}%\")\n",
        "\n",
        "# ----- Plot -----\n",
        "model_names = [r[0] for r in results]\n",
        "accuracies = [r[1] * 100 for r in results]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=accuracies, y=model_names, palette='viridis')\n",
        "plt.title(\"Model Accuracy Comparison\")\n",
        "plt.xlabel(\"Accuracy (%)\")\n",
        "plt.xlim(0, 100)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vlKnxamjxIjp",
        "outputId": "29d40395-6f61-4fe4-d56d-896770ba4398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m143/143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:20:11] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 4680, number of negative: 13576\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 835\n",
            "[LightGBM] [Info] Number of data points in the train set: 18256, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.256354 -> initscore=-1.065005\n",
            "[LightGBM] [Info] Start training from score -1.065005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "<ipython-input-9-38f33efe2658>:157: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=accuracies, y=model_names, palette='viridis')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model Accuracy Comparison (Binary Classification):\n",
            "LogReg: 79.36%\n",
            "RF: 78.09%\n",
            "LogReg + RF: 79.47%\n",
            "SVM + RF: 78.91%\n",
            "ANN + LSTM: 79.56%\n",
            "CNN + LSTM: 79.67%\n",
            "XGB + LGBM: 80.08%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAJOCAYAAACqS2TfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVu1JREFUeJzt3XlYFeX///HXYUdWURQxFBX3LdIwpVySCrdySYvcIc3cJc3MUnPXTMvcDaG+H5c0Tc2PlmZuueRKWilqaq5luQDihjC/P/p5Pp1ARWQ8WM/Hdc2l55577nnPYeryxT2LxTAMQwAAAAAAIM852LsAAAAAAAD+qQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAPx/FotFw4YNu+vtjh07JovFooSEhDyvCchO/fr1Vb9+fXuXAQDIAUI3ACBfSUhIkMVikcVi0bfffptlvWEYCgoKksViUdOmTe1QYd5YuXKlLBaLAgMDlZmZae9yHjgpKSl65513VL16dXl6esrd3V1VqlTRwIEDdfr0aXuXBwCAlZO9CwAAIDtubm6aN2+eHn/8cZv2DRs26OTJk3J1dbVTZXlj7ty5Cg4O1rFjx/TNN98oIiLC3iU9MI4cOaKIiAgdP35crVu3VteuXeXi4qK9e/cqLi5On3/+uQ4ePGjvMk21evVqe5cAAMghZroBAPlS48aNtWjRIt24ccOmfd68eapRo4YCAgLsVNm9S0tL07JlyxQbG6vQ0FDNnTvX3iXdUlpamr1LsHHjxg21bNlSv/32m9avX6/58+erR48e6tKliz788EMdOXJErVu3tneZprl8+bIkycXFRS4uLnauBgCQE4RuAEC+FBUVpXPnzmnNmjXWtuvXr+uzzz7TSy+9lO02aWlpeu211xQUFCRXV1eVL19eEyZMkGEYNv2uXbumfv36yd/fX15eXnr22Wd18uTJbMc8deqUoqOjVbRoUbm6uqpy5cqaM2fOPR3b559/ritXrqh169Z68cUXtWTJEl29ejVLv6tXr2rYsGEqV66c3NzcVKxYMbVs2VI///yztU9mZqY++OADVa1aVW5ubvL391dkZKR27twp6fb3m//9HvZhw4bJYrHop59+0ksvvaSCBQtarzTYu3evOnXqpNKlS8vNzU0BAQGKjo7WuXPnsv3OYmJiFBgYKFdXV5UqVUqvvvqqrl+/riNHjshisWjSpElZttuyZYssFovmz59/y+9u8eLF+v777zV48OAsV0FIkre3t0aNGmXTtmjRItWoUUPu7u4qXLiw2rVrp1OnTtn06dSpkzw9PXX8+HE1bdpUnp6eKl68uKZOnSpJ2rdvn5588kl5eHioZMmSmjdvns32N2+L2Lhxo1555RUVKlRI3t7e6tChgy5cuGDTd9myZWrSpIn1+ylTpoxGjBihjIwMm37169dXlSpVtGvXLtWtW1cFChTQm2++aV3393u6P/zwQ1WuXFkFChRQwYIFVbNmzSx17tmzR40aNZK3t7c8PT3VsGFDbdu2Ldtj2bx5s2JjY+Xv7y8PDw+1aNFCv//+e3Y/FgDAbRC6AQD5UnBwsGrXrm0TwFatWqXk5GS9+OKLWfobhqFnn31WkyZNUmRkpCZOnKjy5ctrwIABio2Nten78ssv6/3339fTTz+tsWPHytnZWU2aNMky5m+//abHHntMX3/9tXr27KkPPvhAISEhiomJ0fvvv5/rY5s7d64aNGiggIAAvfjii0pNTdUXX3xh0ycjI0NNmzbVO++8oxo1aui9995Tnz59lJycrB9++MHaLyYmRn379lVQUJDGjRunN954Q25ublmC1N1o3bq1Ll++rNGjR6tLly6SpDVr1ujIkSPq3LmzPvzwQ7344otasGCBGjdubPNLjdOnTyssLEwLFizQCy+8oMmTJ6t9+/basGGDLl++rNKlSys8PDzb2f25c+fKy8tLzz333C1rW758uSSpffv2OTqWhIQEtWnTRo6OjhozZoy6dOmiJUuW6PHHH9fFixdt+mZkZKhRo0YKCgrS+PHjFRwcrJ49eyohIUGRkZGqWbOmxo0bJy8vL3Xo0EFHjx7Nsr+ePXtq//79GjZsmDp06KC5c+eqefPmNt9RQkKCPD09FRsbqw8++EA1atTQkCFD9MYbb2QZ79y5c2rUqJEefvhhvf/++2rQoEG2xzl79mz17t1blSpV0vvvv6933nlHDz/8sL777jtrnx9//FFPPPGEvv/+e73++ut6++23dfToUdWvX9+m3029evXS999/r6FDh+rVV1/VF198oZ49e+boewcA/IUBAEA+Eh8fb0gyduzYYUyZMsXw8vIyLl++bBiGYbRu3dpo0KCBYRiGUbJkSaNJkybW7ZYuXWpIMkaOHGkz3vPPP29YLBbj8OHDhmEYRmJioiHJ6N69u02/l156yZBkDB061NoWExNjFCtWzPjjjz9s+r744ouGj4+Pta6jR48akoz4+Pg7Ht9vv/1mODk5GbNnz7a21alTx3juueds+s2ZM8eQZEycODHLGJmZmYZhGMY333xjSDJ69+59yz63q+3vxzt06FBDkhEVFZWl781j/av58+cbkoyNGzda2zp06GA4ODgYO3bsuGVNM2fONCQZ+/fvt667fv26UbhwYaNjx45Ztvur0NBQw8fH57Z9/jpmkSJFjCpVqhhXrlyxtq9YscKQZAwZMsTa1rFjR0OSMXr0aGvbhQsXDHd3d8NisRgLFiywth84cCDLd3fzvK1Ro4Zx/fp1a/v48eMNScayZcusbdl9l6+88opRoEAB4+rVq9a2evXqGZKMGTNmZOlfr149o169etbPzz33nFG5cuXbfh/Nmzc3XFxcjJ9//tnadvr0acPLy8uoW7dulmOJiIiw/swMwzD69etnODo6GhcvXrztfgAAtpjpBgDkW23atNGVK1e0YsUKpaamasWKFbe8tHzlypVydHRU7969bdpfe+01GYahVatWWftJytKvb9++Np8Nw9DixYvVrFkzGYahP/74w7o888wzSk5O1u7du+/6mBYsWCAHBwe1atXK2hYVFaVVq1bZXIa8ePFiFS5cWL169coyhsVisfaxWCwaOnToLfvkRrdu3bK0ubu7W/9+9epV/fHHH3rsscckyfo9ZGZmaunSpWrWrJlq1qx5y5ratGkjNzc3m9nur776Sn/88YfatWt329pSUlLk5eWVo+PYuXOnzp49q+7du8vNzc3a3qRJE1WoUEH//e9/s2zz8ssvW//u6+ur8uXLy8PDQ23atLG2ly9fXr6+vjpy5EiW7bt27SpnZ2fr51dffVVOTk7W806y/S5TU1P1xx9/6IknntDly5d14MABm/FcXV3VuXPnOx6rr6+vTp48qR07dmS7PiMjQ6tXr1bz5s1VunRpa3uxYsX00ksv6dtvv1VKSkqWY/nrefTEE08oIyNDv/zyyx3rAQD8D6EbAJBv+fv7KyIiQvPmzdOSJUuUkZGh559/Ptu+v/zyiwIDA7MEsooVK1rX3/zTwcFBZcqUselXvnx5m8+///67Ll68qFmzZsnf399muRmCzp49e9fH9J///EdhYWE6d+6cDh8+rMOHDys0NFTXr1/XokWLrP1+/vlnlS9fXk5Ot37RyM8//6zAwED5+fnddR23U6pUqSxt58+fV58+fVS0aFG5u7vL39/f2i85OVnSn99ZSkqKqlSpctvxfX191axZM5v7jefOnavixYvrySefvO223t7eSk1NzdFx3PyZ//1nK0kVKlTIEh5v3hP/Vz4+PnrooYey/BLDx8cny73aklS2bFmbz56enipWrJiOHTtmbfvxxx/VokUL+fj4yNvbW/7+/tZfNtz8Lm8qXrx4jh6YNnDgQHl6eiosLExly5ZVjx49tHnzZuv633//XZcvX872u6hYsaIyMzN14sQJm/YSJUrYfC5YsKAkZXvcAIBb45VhAIB87aWXXlKXLl3066+/qlGjRvL19b0v+7357ux27dqpY8eO2fapVq3aXY156NAh60zk38OZ9Gfw7Nq1611Wenu3mvH++0O7/uqvM7E3tWnTRlu2bNGAAQP08MMPy9PTU5mZmYqMjMzVe8Y7dOigRYsWacuWLapataqWL1+u7t27y8Hh9vMBFSpU0J49e3TixAkFBQXd9X5vx9HR8a7ajb89oC8nLl68qHr16snb21vDhw9XmTJl5Obmpt27d2vgwIFZvsvsfhbZqVixopKSkrRixQp9+eWXWrx4saZNm6YhQ4bonXfeues6pbw9bgD4NyN0AwDytRYtWuiVV17Rtm3b9Omnn96yX8mSJfX1118rNTXVZrb75uW6JUuWtP6ZmZlpnUm+KSkpyWa8m082z8jIyLN3aM+dO1fOzs76v//7vyyB5ttvv9XkyZN1/PhxlShRQmXKlNF3332n9PR0m8uV/6pMmTL66quvdP78+VvOdt+cnfz7Q8Pu5hLhCxcuaO3atXrnnXc0ZMgQa/uhQ4ds+vn7+8vb29vmQW+3EhkZKX9/f82dO1e1atXS5cuXc/RwtGbNmmn+/Pn6z3/+o0GDBt22782feVJSUpYZ9KSkJOv6vHTo0CGbh51dunRJZ86cUePGjSVJ69ev17lz57RkyRLVrVvX2i+7h7LdLQ8PD73wwgt64YUXdP36dbVs2VKjRo3SoEGD5O/vrwIFCmQ5z6U//xtxcHDI819iAAD+xOXlAIB8zdPTU9OnT9ewYcPUrFmzW/Zr3LixMjIyNGXKFJv2SZMmyWKxqFGjRpJk/XPy5Mk2/f7+NHJHR0e1atVKixcvzjZE5ubVSXPnztUTTzyhF154Qc8//7zNMmDAAEmyPq29VatW+uOPP7Icj/S/mcZWrVrJMIxsZzJv9vH29lbhwoW1ceNGm/XTpk3Lcd03f0Hw9xnOv39nDg4Oat68ub744gvrK8uyq0mSnJycFBUVpYULFyohIUFVq1bN0ZUDzz//vKpWrapRo0Zp69atWdanpqZq8ODBkqSaNWuqSJEimjFjhq5du2bts2rVKu3fvz/bJ9bfq1mzZik9Pd36efr06bpx44b1vMvuu7x+/fpd/Tyy8/dXt7m4uKhSpUoyDEPp6elydHTU008/rWXLltlc6v7bb79p3rx5evzxx+Xt7X1PNQAAssdMNwAg37vV5d1/1axZMzVo0ECDBw/WsWPHVL16da1evVrLli1T3759rfdwP/zww4qKitK0adOUnJysOnXqaO3atTp8+HCWMceOHat169apVq1a6tKliypVqqTz589r9+7d+vrrr3X+/PkcH8N3332nw4cP3/KVS8WLF9cjjzyiuXPnauDAgerQoYM++eQTxcbGavv27XriiSeUlpamr7/+Wt27d9dzzz2nBg0aqH379po8ebIOHTpkvdR706ZNatCggXVfL7/8ssaOHauXX35ZNWvW1MaNG3Xw4MEc1+7t7a26detq/PjxSk9PV/HixbV69epsZ2dHjx6t1atXq169euratasqVqyoM2fOaNGiRfr2229tbg/o0KGDJk+erHXr1mncuHE5qsXZ2VlLlixRRESE6tatqzZt2ig8PFzOzs768ccfNW/ePBUsWFCjRo2Ss7Ozxo0bp86dO6tevXqKiorSb7/9pg8++EDBwcHq169fjr+DnLp+/boaNmyoNm3aKCkpSdOmTdPjjz+uZ599VpJUp04dFSxYUB07dlTv3r1lsVj0f//3f/d8yfbTTz+tgIAAhYeHq2jRotq/f7+mTJmiJk2aWK/8GDlypNasWaPHH39c3bt3l5OTk2bOnKlr165p/Pjx93zsAIBbsMsz0wEAuIW/vjLsdv7+yjDDMIzU1FSjX79+RmBgoOHs7GyULVvWePfdd21ee2QYhnHlyhWjd+/eRqFChQwPDw+jWbNmxokTJ7K8Bsow/nzFV48ePYygoCDD2dnZCAgIMBo2bGjMmjXL2icnrwzr1auXIcnmdU1/N2zYMEOS8f333xuG8eerpQYPHmyUKlXKuu/nn3/eZowbN24Y7777rlGhQgXDxcXF8Pf3Nxo1amTs2rXL2ufy5ctGTEyM4ePjY3h5eRlt2rQxzp49e8tXhv3+++9Zajt58qTRokULw9fX1/Dx8TFat25tnD59Otvv7JdffjE6dOhg+Pv7G66urkbp0qWNHj16GNeuXcsybuXKlQ0HBwfj5MmTt/xesnPhwgVjyJAhRtWqVY0CBQoYbm5uRpUqVYxBgwYZZ86csen76aefGqGhoYarq6vh5+dntG3bNsv+OnbsaHh4eGTZT7169bJ9Fdffz7+b5+2GDRuMrl27GgULFjQ8PT2Ntm3bGufOnbPZdvPmzcZjjz1muLu7G4GBgcbrr79ufPXVV4YkY926dXfc9811f31l2MyZM426desahQoVMlxdXY0yZcoYAwYMMJKTk2222717t/HMM88Ynp6eRoECBYwGDRoYW7Zsselzq/8G161bl6VGAMCdWQyDp2EAAAD7CA0NlZ+fn9auXWvvUu5JQkKCOnfurB07dmT7ujQAwL8X93QDAAC72LlzpxITE9WhQwd7lwIAgGm4pxsAANxXP/zwg3bt2qX33ntPxYoV0wsvvGDvkgAAMA0z3QAA4L767LPP1LlzZ6Wnp2v+/Plyc3Ozd0kAAJiGe7oBAAAAADAJM90AAAAAAJiE0A0AAAAAgEl4kNoDKDMzU6dPn5aXl5csFou9ywEAAACAB5phGEpNTVVgYKAcHPJ2bprQ/QA6ffq0goKC7F0GAAAAAPyjnDhxQg899FCejknofgB5eXlJ+vOE8Pb2tnM1AAAAAPBgS0lJUVBQkDVr5SVC9wPo5iXl3t7ehG4AAAAAyCNm3L7Lg9QAAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCU8vf4A9/+QgOTu52rsMAAAAQP/dNtHeJQD5EjPdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmOQfE7o7deqk5s2bmzZ+/fr1ZbFYZLFY5ObmpnLlymnMmDEyDMO0fQIAAAAAHmz/mNB9P3Tp0kVnzpxRUlKSBg0apCFDhmjGjBn2LgsAAAAAkE/9K0L3hg0bFBYWJldXVxUrVkxvvPGGbty4YV2fmpqqtm3bysPDQ8WKFdOkSZNUv3599e3b12acAgUKKCAgQCVLllTnzp1VrVo1rVmzxrr+2rVr6t+/v4oXLy4PDw/VqlVL69evtxlj9uzZCgoKUoECBdSiRQtNnDhRvr6+Jh49AAAAAMBe/vGh+9SpU2rcuLEeffRRff/995o+fbri4uI0cuRIa5/Y2Fht3rxZy5cv15o1a7Rp0ybt3r37lmMahqFNmzbpwIEDcnFxsbb37NlTW7du1YIFC7R37161bt1akZGROnTokCRp8+bN6tatm/r06aPExEQ99dRTGjVq1B2P4dq1a0pJSbFZAAAAAAD5n5O9CzDbtGnTFBQUpClTpshisahChQo6ffq0Bg4cqCFDhigtLU0ff/yx5s2bp4YNG0qS4uPjFRgYmO1YH330ka5fv6709HS5ubmpd+/ekqTjx48rPj5ex48ft27bv39/ffnll4qPj9fo0aP14YcfqlGjRurfv78kqVy5ctqyZYtWrFhx22MYM2aM3nnnnbz8WgAAAAAA98E/fqZ7//79ql27tiwWi7UtPDxcly5d0smTJ3XkyBGlp6crLCzMut7Hx0fly5fPMlbbtm2VmJiozZs3q1GjRho8eLDq1KkjSdq3b58yMjJUrlw5eXp6WpcNGzbo559/liQlJSXZ7EdSls/ZGTRokJKTk63LiRMncvVdAAAAAADur3/8THde8vHxUUhIiCRp4cKFCgkJ0WOPPaaIiAhdunRJjo6O2rVrlxwdHW228/T0vKf9urq6ytXV9Z7GAAAAAADcf//4me6KFStq69atNq/22rx5s7y8vPTQQw+pdOnScnZ21o4dO6zrk5OTdfDgwduO6+npqT59+qh///4yDEOhoaHKyMjQ2bNnFRISYrMEBARIksqXL2+zH0lZPgMAAAAA/jn+UaE7OTlZiYmJNkvXrl114sQJ9erVSwcOHNCyZcs0dOhQxcbGysHBQV5eXurYsaMGDBigdevW6ccff1RMTIwcHBxsLknPziuvvKKDBw9q8eLFKleunNq2basOHTpoyZIlOnr0qLZv364xY8bov//9rySpV69eWrlypSZOnKhDhw5p5syZWrVq1R33AwAAAAB4MP2jQvf69esVGhpqs4wYMUIrV67U9u3bVb16dXXr1k0xMTF66623rNtNnDhRtWvXVtOmTRUREaHw8HBVrFhRbm5ut92fn5+fOnTooGHDhikzM1Px8fHq0KGDXnvtNZUvX17NmzfXjh07VKJECUl/3ks+Y8YMTZw4UdWrV9eXX36pfv363XE/AAAAAIAHk8X463XXkCSlpaWpePHieu+99xQTE2Pqvrp06aIDBw5o06ZNOd4mJSVFPj4+eqpGdzk7ca83AAAA7O+/2ybauwQg125mrOTkZHl7e+fp2DxITdKePXt04MABhYWFKTk5WcOHD5ckPffcc3m+rwkTJuipp56Sh4eHVq1apY8//ljTpk3L8/0AAAAAAOyP0P3/TZgwQUlJSXJxcVGNGjW0adMmFS5cOM/3s337do0fP16pqakqXbq0Jk+erJdffjnP9wMAAAAAsD9Ct6TQ0FDt2rXrvuxr4cKF92U/AAAAAAD7+0c9SA0AAAAAgPyE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJjEyd4FIPc++2aMvL297V0GAAAAAOAWmOkGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAEziZO8CkHvNO42Vk7ObvcsAAADAP9TqT4fYuwTggcdMNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdOexTp06yWKxyGKxyNnZWaVKldLrr7+uq1evWvvcXP/X5fHHH7dj1QAAAAAAMzjZu4B/osjISMXHxys9PV27du1Sx44dZbFYNG7cOGuf+Ph4RUZGWj+7uLjYo1QAAAAAgIkI3SZwdXVVQECAJCkoKEgRERFas2aNTej29fW19gEAAAAA/DMRuk32ww8/aMuWLSpZsmSux7h27ZquXbtm/ZySkpIXpQEAAAAATMY93SZYsWKFPD095ebmpqpVq+rs2bMaMGCATZ+oqCh5enpal6VLl95yvDFjxsjHx8e6BAUFmXwEAAAAAIC8wEy3CRo0aKDp06crLS1NkyZNkpOTk1q1amXTZ9KkSYqIiLB+Llas2C3HGzRokGJjY62fU1JSCN4AAAAA8AAgdJvAw8NDISEhkqQ5c+aoevXqiouLU0xMjLVPQECAtc+duLq6ytXV1ZRaAQAAAADm4fJykzk4OOjNN9/UW2+9pStXrti7HAAAAADAfUTovg9at24tR0dHTZ061d6lAAAAAADuI0L3feDk5KSePXtq/PjxSktLs3c5AAAAAID7xGIYhmHvInB3UlJS5OPjowYtBsnJ2c3e5QAAAOAfavWnQ+xdAnBf3MxYycnJ8vb2ztOxmekGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATOJk7wKQe0sT3pC3t7e9ywAAAAAA3AIz3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASZzsXQByr+Hr4+Tk4mbvMgAAAABJ0tbJb9u7BCDfYaYbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAk9x16O7UqZOaN29uQin/U79+fVksFlksFrm5ualcuXIaM2aMDMMwdb954WbdFotF3t7eevTRR7Vs2TKbPgkJCTb9bi4fffSRnaoGAAAAAJgh3850d+nSRWfOnFFSUpIGDRqkIUOGaMaMGfe1hmPHjslisdz1dvHx8Tpz5ox27typ8PBwPf/889q3b59NH29vb505c8Zmadu2bV6VDgAAAADIB/I8dG/YsEFhYWFydXVVsWLF9MYbb+jGjRvW9ampqWrbtq08PDxUrFgxTZo0SfXr11ffvn1txilQoIACAgJUsmRJde7cWdWqVdOaNWus669du6b+/furePHi8vDwUK1atbR+/XqbMWbPnq2goCAVKFBALVq00MSJE+Xr65vXh5yFr6+vAgICVK5cOY0YMUI3btzQunXrbPpYLBYFBATYLO7u7qbXBgAAAAC4f/I0dJ86dUqNGzfWo48+qu+//17Tp09XXFycRo4cae0TGxurzZs3a/ny5VqzZo02bdqk3bt333JMwzC0adMmHThwQC4uLtb2nj17auvWrVqwYIH27t2r1q1bKzIyUocOHZIkbd68Wd26dVOfPn2UmJiop556SqNGjcrLw72jGzduKC4uTpJsar9b165dU0pKis0CAAAAAMj/nPJysGnTpikoKEhTpkyRxWJRhQoVdPr0aQ0cOFBDhgxRWlqaPv74Y82bN08NGzaU9Oel2IGBgdmO9dFHH+n69etKT0+Xm5ubevfuLUk6fvy44uPjdfz4ceu2/fv315dffqn4+HiNHj1aH374oRo1aqT+/ftLksqVK6ctW7ZoxYoVeXnI2YqKipKjo6OuXLmizMxMBQcHq02bNjZ9kpOT5enpaf3s6empX3/9NdvxxowZo3feecfUmgEAAAAAeS9PQ/f+/ftVu3Ztm/ugw8PDdenSJZ08eVIXLlxQenq6wsLCrOt9fHxUvnz5LGO1bdtWgwcP1oULFzR06FDVqVNHderUkSTt27dPGRkZKleunM02165dU6FChSRJSUlJatGihc36sLCwO4buypUr65dffpEk64Pb/hqOn3jiCa1ateq2Y0yaNEkRERE6cuSI+vXrp8mTJ8vPz8+mj5eXl80Mv4PDrS86GDRokGJjY62fU1JSFBQUdNsaAAAAAAD2l6ehOy/5+PgoJCREkrRw4UKFhIToscceU0REhC5duiRHR0ft2rVLjo6ONtv9NSDnxsqVK5Weni7pz8vl69evr8TEROv6nNx3HRAQoJCQEIWEhCg+Pl6NGzfWTz/9pCJFilj7ODg4WI/vTlxdXeXq6np3BwIAAAAAsLs8Dd0VK1bU4sWLZRiGdbZ78+bN8vLy0kMPPaSCBQvK2dlZO3bsUIkSJST9eZn1wYMHVbdu3VuO6+npqT59+qh///7as2ePQkNDlZGRobNnz+qJJ57Idpvy5ctrx44dNm1//5ydkiVLWv/u5PTn15PTcJydsLAw1ahRQ6NGjdIHH3yQ63EAAAAAAA+eXD1ILTk5WYmJiTbLiRMn1L17d504cUK9evXSgQMHtGzZMg0dOlSxsbFycHCQl5eXOnbsqAEDBmjdunX68ccfFRMTIwcHhzu+muuVV17RwYMHtXjxYpUrV05t27ZVhw4dtGTJEh09elTbt2/XmDFj9N///leS1KtXL61cuVITJ07UoUOHNHPmTK1atSpXrwC7V3379tXMmTN16tSp+75vAAAAAID95Cp0r1+/XqGhoTbLO++8o+LFi2vlypXavn27qlevrm7duikmJkZvvfWWdduJEyeqdu3aatq0qSIiIhQeHq6KFSvKzc3ttvv08/NThw4dNGzYMGVmZio+Pl4dOnTQa6+9pvLly6t58+Y2M+jh4eGaMWOGJk6cqOrVq+vLL79Uv3797rgfM0RGRqpUqVL3/enpAAAAAAD7shg3nxZmJ2lpaSpevLjee+89xcTEmLqvLl266MCBA9q0aZOp+zFbSkqKfHx8VPOVN+Xkcv9/iQAAAABkZ+vkt+1dApArNzNWcnKyvL2983Ts+/4gtT179ujAgQMKCwtTcnKyhg8fLkl67rnn8nxfEyZM0FNPPSUPDw+tWrVKH3/8saZNm5bn+wEAAAAAIDt2eXr5hAkTlJSUJBcXF9WoUUObNm1S4cKF83w/27dv1/jx45WamqrSpUtr8uTJevnll/N8PwAAAAAAZOe+h+7Q0FDt2rXrvuxr4cKF92U/AAAAAABkJ1cPUgMAAAAAAHdG6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAEziZO8CkHtrxw+Ut7e3vcsAAAAAANwCM90AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEmc7F0Aci98/Bg5urnauwwAAAD8CyW+NczeJQAPBGa6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMEm+CN2///67Xn31VZUoUUKurq4KCAjQM888o82bN+v69esqXLiwxo4dm+22I0aMUNGiRZWenq6EhARZLBZVrFgxS79FixbJYrEoODjY1GOxWCzWxdvbW48++qiWLVtm0+dmnX9fPvroI1NrAwAAAADcX/kidLdq1Up79uzRxx9/rIMHD2r58uWqX7++zp07JxcXF7Vr107x8fFZtjMMQwkJCerQoYOcnZ0lSR4eHjp79qy2bt1q0zcuLk4lSpS4q7qOHTsmi8Vy18cTHx+vM2fOaOfOnQoPD9fzzz+vffv22fTx9vbWmTNnbJa2bdve9b4AAAAAAPmX3UP3xYsXtWnTJo0bN04NGjRQyZIlFRYWpkGDBunZZ5+VJMXExOjgwYP69ttvbbbdsGGDjhw5opiYGGubk5OTXnrpJc2ZM8fadvLkSa1fv14vvfTSfTkmX19fBQQEqFy5choxYoRu3LihdevW2fSxWCwKCAiwWdzd3e9LfQAAAACA+8PuodvT01Oenp5aunSprl27lm2fqlWr6tFHH7UJ0tKfM8p16tRRhQoVbNqjo6O1cOFCXb58WdKfl3NHRkaqaNGi5hzELdy4cUNxcXGSJBcXl/u6bwAAAACA/dk9dDs5OSkhIUEff/yxfH19FR4erjfffFN79+616RcTE6NFixbp0qVLkqTU1FR99tlnio6OzjJmaGioSpcurc8++8x6CXp2/cwSFRUlT09Pubq6ql+/fgoODlabNm1s+iQnJ1t/4eDp6amAgIBbjnft2jWlpKTYLAAAAACA/M/uoVv6857u06dPa/ny5YqMjNT69ev1yCOPKCEhwdonKipKGRkZWrhwoSTp008/lYODg1544YVsx4yOjlZ8fLw2bNigtLQ0NW7cOEe1VK5c2RqEK1euLEk24bhRo0Z3HGPSpElKTEzUqlWrVKlSJX300Ufy8/Oz6ePl5aXExETrsmXLlluON2bMGPn4+FiXoKCgHB0LAAAAAMC+nOxdwE1ubm566qmn9NRTT+ntt9/Wyy+/rKFDh6pTp06S/nzw2PPPP6/4+HhroG7Tpo08PT2zHa9t27Z6/fXXNWzYMLVv315OTjk71JUrVyo9PV2SdOrUKdWvX1+JiYnW9Tm57zogIEAhISEKCQlRfHy8GjdurJ9++klFihSx9nFwcFBISEiOaho0aJBiY2Otn1NSUgjeAAAAAPAAyDeh++8qVaqkpUuX2rTFxMSofv36WrFihbZs2aJ33333ltv7+fnp2Wef1cKFCzVjxowc77dkyZLWv98M6jkNx9kJCwtTjRo1NGrUKH3wwQe5GsPV1VWurq65rgEAAAAAYB92v7z83LlzevLJJ/Wf//xHe/fu1dGjR7Vo0SKNHz9ezz33nE3funXrKiQkRB06dFCFChVUp06d246dkJCgP/74I8uD1u63vn37aubMmTp16pRd6wAAAAAA3F92D92enp6qVauWJk2apLp166pKlSp6++231aVLF02ZMsWmr8ViUXR0tC5cuJCjB6O5u7urUKFCZpWeY5GRkSpVqpRGjRpl71IAAAAAAPeRxTAMw95F4O6kpKTIx8dHVQa/IUc3LjsHAADA/Zf41jB7lwDkmZsZKzk5Wd7e3nk6tt1nugEAAAAA+KcidAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEid7F4Dc2/z6IHl7e9u7DAAAAADALTDTDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYhNANAAAAAIBJCN0AAAAAAJiE0A0AAAAAgEkI3QAAAAAAmITQDQAAAACASQjdAAAAAACYxMneBSD3Iv4zQk7urvYuAwAAALDa0nmkvUsA8hVmugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADDJPYfurVu3ytHRUU2aNMmy7tixY7JYLCpSpIhSU1Nt1j388MMaNmyY9XP9+vVlsVi0YMECm37vv/++goOD77XM27JYLFq6dOkt18+ePVvVq1eXp6enfH19FRoaqjFjxkiSgoODZbFYbrl06tTJug+LxaJt27bZjH3t2jUVKlRIFotF69evN+kIAQAAAAD2cM+hOy4uTr169dLGjRt1+vTpbPukpqZqwoQJdxzLzc1Nb731ltLT03Ndz82gn1fmzJmjvn37qnfv3kpMTNTmzZv1+uuv69KlS5KkHTt26MyZMzpz5owWL14sSUpKSrK2ffDBB9axgoKCFB8fbzP+559/Lk9PzzyrFwAAAACQf9xT6L506ZI+/fRTvfrqq2rSpIkSEhKy7derVy9NnDhRZ8+eve14UVFRunjxombPnn0vZeWp5cuXq02bNoqJiVFISIgqV66sqKgojRo1SpLk7++vgIAABQQEyM/PT5JUpEgRa5uPj491rI4dO2rBggW6cuWKtW3OnDnq2LHj/T0oAAAAAMB9cU+he+HChapQoYLKly+vdu3aac6cOTIMI0u/qKgohYSEaPjw4bcdz9vbW4MHD9bw4cOVlpZ2L6XlmYCAAG3btk2//PLLPY9Vo0YNBQcHW2fEjx8/ro0bN6p9+/b3PDYAAAAAIP+5p9AdFxendu3aSZIiIyOVnJysDRs2ZOlnsVg0duxYzZo1Sz///PNtx+zevbvc3Nw0ceLEeyktzwwdOlS+vr4KDg5W+fLl1alTJy1cuFCZmZm5Gi86Olpz5syRJCUkJKhx48by9/e/7TbXrl1TSkqKzQIAAAAAyP9yHbqTkpK0fft2RUVFSZKcnJz0wgsvKC4uLtv+zzzzjB5//HG9/fbbtx3X1dVVw4cP14QJE/THH3/kqJbKlSvL09NTnp6eqly5siRZP3t6eqpRo0Z3cWS2ihUrpq1bt2rfvn3q06ePbty4oY4dOyoyMjJXwbtdu3baunWrjhw5ooSEBEVHR99xmzFjxsjHx8e6BAUF5eZQAAAAAAD3mVNuN4yLi9ONGzcUGBhobTMMQ66urpoyZYrNvcw3jR07VrVr19aAAQNuO3a7du00YcIEjRw5MkdPLl+5cqX14WunTp1S/fr1lZiYaF3v7u6es4O6jSpVqqhKlSrq3r27unXrpieeeEIbNmxQgwYN7mqcQoUKqWnTpoqJidHVq1fVqFGjLE92/7tBgwYpNjbW+jklJYXgDQAAAAAPgFyF7hs3buiTTz7Re++9p6efftpmXfPmzTV//nx169Yty3ZhYWFq2bKl3njjjduO7+DgoDFjxqhly5Z69dVX71hPyZIlrX93cvrzkEJCQnJyKLlSqVIlScr1fefR0dFq3LixBg4cKEdHxzv2d3V1laura672BQAAAACwn1yF7hUrVujChQuKiYnJMqPdqlUrxcXFZRu6JWnUqFGqXLmyNRzfSpMmTVSrVi3NnDlTRYsWzU2Zd+Xo0aM2s+OSVLZsWfXv31+BgYF68skn9dBDD+nMmTMaOXKk/P39Vbt27VztKzIyUr///ru8vb3zoHIAAAAAQH6Vq3u64+LiFBERke0l5K1atdLOnTu1d+/ebLctV66coqOjdfXq1TvuZ9y4cTnqlxdiY2MVGhpqs+zZs0cRERHatm2bWrdurXLlyqlVq1Zyc3PT2rVrVahQoVzty2KxqHDhwnJxccnjowAAAAAA5CcWI7t3fCFfS0lJkY+Pjx6d2l9O7lx2DgAAgPxjS+eR9i4BuGs3M1ZycnKeX5F8T68MAwAAAAAAt0boBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkTvYuALn3dbu35e3tbe8yAAAAAAC3wEw3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASJ3sXgNwbsP51uXi42rsMAAAAwMaHDT+wdwlAvsFMNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACYhdAMAAAAAYBJCNwAAAAAAJiF0AwAAAABgEkI3AAAAAAAmIXQDAAAAAGASQjcAAAAAACbJdej+9ddf1atXL5UuXVqurq4KCgpSs2bNtHbtWmuf4OBgWSwWbdu2zWbbvn37qn79+tbPw4YNk8ViUbdu3Wz6JSYmymKx6NixY7kt847q16+vvn373nL9hg0b9OSTT8rPz08FChRQ2bJl1bFjR12/fl2dOnWSxWK55RIcHGzdh8Vi0dixY7OM36RJE1ksFg0bNsycAwQAAAAA2E2uQvexY8dUo0YNffPNN3r33Xe1b98+ffnll2rQoIF69Ohh09fNzU0DBw6845hubm6Ki4vToUOHclOSVXBwsNavX39PY9z0008/KTIyUjVr1tTGjRu1b98+ffjhh3JxcVFGRoY++OADnTlzxrpIUnx8vPXzjh07rGMFBQUpISHBZvxTp05p7dq1KlasWJ7UCwAAAADIX5xys1H37t1lsVi0fft2eXh4WNsrV66s6Ohom75du3bVjBkztHLlSjVu3PiWY5YvX15FihTR4MGDtXDhwtyUledWr16tgIAAjR8/3tpWpkwZRUZGSpLc3d3l4+Njs42vr68CAgKyjNW0aVMtXLhQmzdvVnh4uCTp448/1tNPP63jx4+beBQAAAAAAHu565nu8+fP68svv1SPHj1sAvdNvr6+Np9LlSqlbt26adCgQcrMzLzt2GPHjtXixYu1c+fOuy3LFAEBATpz5ow2btx4z2O5uLiobdu2io+Pt7YlJCRk+SUFAAAAAOCf465D9+HDh2UYhipUqJDjbd566y0dPXpUc+fOvW2/Rx55RG3atMnR5ej3Q+vWrRUVFaV69eqpWLFiatGihaZMmaKUlJRcjRcdHa2FCxcqLS1NGzduVHJyspo2bXrH7a5du6aUlBSbBQAAAACQ/9116DYM46534u/vr/79+2vIkCG6fv36bfuOHDlSmzZt0urVq3M0drdu3eTp6Wldjh8/rkaNGtm05Zajo6Pi4+N18uRJjR8/XsWLF9fo0aNVuXJl6z3cd6N69eoqW7asPvvsM82ZM0ft27eXk9Odr/AfM2aMfHx8rEtQUFBuDgcAAAAAcJ/ddeguW7asLBaLDhw4cFfbxcbG6sqVK5o2bdpt+5UpU0ZdunTRG2+8kaOAP3z4cCUmJlqXwMBAffTRRzZt96p48eJq3769pkyZoh9//FFXr17VjBkzcjVWdHS0pk6dqs8++yzHl5YPGjRIycnJ1uXEiRO52jcAAAAA4P6669Dt5+enZ555RlOnTlVaWlqW9RcvXsx2O09PT7399tsaNWqUUlNTb7uPIUOG6ODBg1qwYMEd6ylSpIhCQkKsi5OTk4oXL27TlpcKFiyoYsWKZXvsOfHSSy9p3759qlKliipVqpSjbVxdXeXt7W2zAAAAAADyv1w9vXzq1KkKDw9XWFiYhg8frmrVqunGjRtas2aNpk+frv3792e7XdeuXTVp0iTNmzdPtWrVuuX4RYsWVWxsrN59993clHfXfv/99ywz4sWKFdPSpUuVmJioFi1aqEyZMrp69ao++eQT/fjjj/rwww9zta+CBQvqzJkzcnZ2zoPKAQAAAAD5Wa7e0126dGnt3r1bDRo00GuvvaYqVaroqaee0tq1azV9+vRbbufs7KwRI0bo6tWrd9xH//797+l+7Lsxb948hYaG2iyzZ89WWFiYLl26pG7duqly5cqqV6+etm3bpqVLl6pevXq53p+vr2+2T34HAAAAAPyzWIzcPBkNdpWSkiIfHx91XfaKXDxc7V0OAAAAYOPDhh/YuwTgrtzMWMnJyXl+O2+uZroBAAAAAMCdEboBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMAmhGwAAAAAAkxC6AQAAAAAwCaEbAAAAAACTELoBAAAAADAJoRsAAAAAAJMQugEAAAAAMImTvQtA7r1bf7y8vb3tXQYAAAAA4BaY6QYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATOJk7wKQe5/uaCF3D36EAAAAyL/aPfaVvUsA7IqZbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAEyS49CdkZGhOnXqqGXLljbtycnJCgoK0uDBg23aFy9erCeffFIFCxaUu7u7ypcvr+joaO3Zs8faJyEhQRaLxbp4enqqRo0aWrJkyT0e1u0lJCTI19f3tn1+/fVX9enTRyEhIXJzc1PRokUVHh6u6dOn6/Lly9Z+wcHB1vodHR0VGBiomJgYXbhwwdpn/fr1slgsKliwoK5evWqznx07dli3BwAAAAD8s+Q4dDs6OiohIUFffvml5s6da23v1auX/Pz8NHToUGvbwIED9cILL+jhhx/W8uXLlZSUpHnz5ql06dIaNGiQzbje3t46c+aMzpw5oz179uiZZ55RmzZtlJSUlOODGDZsmDp16pTj/ndy5MgRhYaGavXq1Ro9erT27NmjrVu36vXXX9eKFSv09ddf2/QfPny4zpw5o+PHj2vu3LnauHGjevfunWVcLy8vff755zZtcXFxKlGiRJ7VDgAAAADIP5zupnO5cuU0duxY9erVS08++aS2b9+uBQsWaMeOHXJxcZEkbdu2TePHj9cHH3xgEzxLlCihGjVqyDAMmzEtFosCAgIkSQEBARo5cqQmTJigvXv3qnz58vd6fLnSvXt3OTk5aefOnfLw8LC2ly5dWs8991yWY/Dy8rIeQ/HixdWxY0fNnz8/y7gdO3bUnDlzFBUVJUm6cuWKFixYoN69e2vEiBEmHhEAAAAAwB7u+p7uXr16qXr16mrfvr26du2qIUOGqHr16tb18+fPl6enp7p3757t9re7jDojI0Mff/yxJOmRRx6529LyxLlz57R69Wr16NHDJnD/1e2O4dSpU/riiy9Uq1atLOvat2+vTZs26fjx45L+vAQ/ODjYbscKAAAAADDXXYdui8Wi6dOna+3atSpatKjeeOMNm/UHDx5U6dKl5eT0v0n0iRMnytPT07okJydb1yUnJ1vbXVxc9Oqrr2rWrFkqU6bMPRxW7h0+fFiGYWSZZS9cuLC1zoEDB9qsGzhwoDw9PeXu7q6HHnpIFotFEydOzDJ2kSJF1KhRIyUkJEiS5syZo+jo6DvWdO3aNaWkpNgsAAAAAID8L1dPL58zZ44KFCigo0eP6uTJk3fsHx0drcTERM2cOVNpaWk2l2d7eXkpMTFRiYmJ2rNnj0aPHq1u3brpiy++uOV4mzZtsgnxo0eP1ty5c23a/nrfeV7Yvn27EhMTVblyZV27ds1m3YABA5SYmKi9e/dq7dq1kqQmTZooIyMjyzjR0dFKSEjQkSNHtHXrVrVt2/aO+x4zZox8fHysS1BQUN4cFAAAAADAVHd1T7ckbdmyRZMmTdLq1as1cuRIxcTE6Ouvv7Zecl22bFl9++23Sk9Pl7OzsyTJ19dXvr6+2QZ0BwcHhYSEWD9Xq1ZNq1ev1rhx49SsWbNsa6hZs6YSExOtnydPnqxTp05p3Lhx1raiRYve7aFJkkJCQmSxWLI8yK106dKSJHd39yzbFC5c2HoMZcuW1fvvv6/atWtr3bp1ioiIsOnbqFEjde3aVTExMWrWrJkKFSp0x5oGDRqk2NhY6+eUlBSCNwAAAAA8AO5qpvvy5cvq1KmTXn31VTVo0EBxcXHavn27ZsyYYe0TFRWlS5cuadq0abkuytHRUVeuXLnlend3d4WEhFgXPz8/eXl52bR5eXnlat+FChXSU089pSlTpigtLS3X9UvK9hicnJzUoUMHrV+/PkeXlkuSq6urvL29bRYAAAAAQP53VzPdgwYNkmEYGjt2rKQ/31E9YcIE9e/fX40aNVJwcLBq166t1157Ta+99pp++eUXtWzZUkFBQTpz5ozi4uJksVjk4PC/rG8Yhn799VdJf4bUNWvW6KuvvtKQIUPy8DCzysjIsJktl/4MtxUrVtS0adMUHh6umjVratiwYapWrZocHBy0Y8cOHThwQDVq1LDZLjU1Vb/++qsMw9CJEyf0+uuvy9/fX3Xq1Ml23yNGjNCAAQNyNMsNAAAAAHhw5Th0b9iwQVOnTtX69etVoEABa/srr7yiJUuW2FxmPmHCBIWFhWn69OmaM2eOLl++rKJFi6pu3braunWrzUxtSkqKihUrJunP0FuyZEkNHz48y8PK8tqlS5cUGhpq01amTBkdPnxYZcqUsd5fPmjQIJ08eVKurq6qVKmS+vfvn+XJ7EOGDLH+ksDf31+PPvqoVq9efctQ7eLiosKFC5tzYAAAAACAfMNi/P2l08j3UlJS5OPjo1lfPyl3j7u+LR8AAAC4b9o99pW9SwDu6GbGSk5OzvPbeXP19HIAAAAAAHBnhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJE72LgC598Kjn8vb29veZQAAAAAAboGZbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAEzC08sfQIZhSJJSUlLsXAkAAAAAPPhuZqubWSsvEbofQOfOnZMkBQUF2bkSAAAAAPjnOHfunHx8fPJ0TEL3A8jPz0+SdPz48Tw/IYC8lJKSoqCgIJ04cYJ3yiPf43zFg4JzFQ8KzlU8SJKTk1WiRAlr1spLhO4HkIPDn7fi+/j48D8wPBC8vb05V/HA4HzFg4JzFQ8KzlU8SG5mrTwdM89HBAAAAAAAkgjdAAAAAACYhtD9AHJ1ddXQoUPl6upq71KA2+JcxYOE8xUPCs5VPCg4V/EgMfN8tRhmPBMdAAAAAAAw0w0AAAAAgFkI3QAAAAAAmITQDQAAAACASQjdD6CpU6cqODhYbm5uqlWrlrZv327vkvAvN2bMGD366KPy8vJSkSJF1Lx5cyUlJdn0uXr1qnr06KFChQrJ09NTrVq10m+//WanioE/jR07VhaLRX379rW2ca4ivzh16pTatWunQoUKyd3dXVWrVtXOnTut6w3D0JAhQ1SsWDG5u7srIiJChw4dsmPF+LfKyMjQ22+/rVKlSsnd3V1lypTRiBEj9NdHR3G+wh42btyoZs2aKTAwUBaLRUuXLrVZn5Pz8vz582rbtq28vb3l6+urmJgYXbp06a7qIHQ/YD799FPFxsZq6NCh2r17t6pXr65nnnlGZ8+etXdp+BfbsGGDevTooW3btmnNmjVKT0/X008/rbS0NGuffv366YsvvtCiRYu0YcMGnT59Wi1btrRj1fi327Fjh2bOnKlq1arZtHOuIj+4cOGCwsPD5ezsrFWrVumnn37Se++9p4IFC1r7jB8/XpMnT9aMGTP03XffycPDQ88884yuXr1qx8rxbzRu3DhNnz5dU6ZM0f79+zVu3DiNHz9eH374obUP5yvsIS0tTdWrV9fUqVOzXZ+T87Jt27b68ccftWbNGq1YsUIbN25U165d764QAw+UsLAwo0ePHtbPGRkZRmBgoDFmzBg7VgXYOnv2rCHJ2LBhg2EYhnHx4kXD2dnZWLRokbXP/v37DUnG1q1b7VUm/sVSU1ONsmXLGmvWrDHq1atn9OnTxzAMzlXkHwMHDjQef/zxW67PzMw0AgICjHfffdfadvHiRcPV1dWYP3/+/SgRsGrSpIkRHR1t09ayZUujbdu2hmFwviJ/kGR8/vnn1s85OS9/+uknQ5KxY8cOa59Vq1YZFovFOHXqVI73zUz3A+T69evatWuXIiIirG0ODg6KiIjQ1q1b7VgZYCs5OVmS5OfnJ0natWuX0tPTbc7dChUqqESJEpy7sIsePXqoSZMmNuekxLmK/GP58uWqWbOmWrdurSJFiig0NFSzZ8+2rj969Kh+/fVXm3PVx8dHtWrV4lzFfVenTh2tXbtWBw8elCR9//33+vbbb9WoUSNJnK/In3JyXm7dulW+vr6qWbOmtU9ERIQcHBz03Xff5XhfTnlXNsz2xx9/KCMjQ0WLFrVpL1q0qA4cOGCnqgBbmZmZ6tu3r8LDw1WlShVJ0q+//ioXFxf5+vra9C1atKh+/fVXO1SJf7MFCxZo9+7d2rFjR5Z1nKvIL44cOaLp06crNjZWb775pnbs2KHevXvLxcVFHTt2tJ6P2f2bgHMV99sbb7yhlJQUVahQQY6OjsrIyNCoUaPUtm1bSeJ8Rb6Uk/Py119/VZEiRWzWOzk5yc/P767OXUI3gDzVo0cP/fDDD/r222/tXQqQxYkTJ9SnTx+tWbNGbm5u9i4HuKXMzEzVrFlTo0ePliSFhobqhx9+0IwZM9SxY0c7VwfYWrhwoebOnat58+apcuXKSkxMVN++fRUYGMj5CogHqT1QChcuLEdHxyxP0f3tt98UEBBgp6qA/+nZs6dWrFihdevW6aGHHrK2BwQE6Pr167p48aJNf85d3G+7du3S2bNn9cgjj8jJyUlOTk7asGGDJk+eLCcnJxUtWpRzFflCsWLFVKlSJZu2ihUr6vjx45JkPR/5NwHygwEDBuiNN97Qiy++qKpVq6p9+/bq16+fxowZI4nzFflTTs7LgICALA+svnHjhs6fP39X5y6h+wHi4uKiGjVqaO3atda2zMxMrV27VrVr17ZjZfi3MwxDPXv21Oeff65vvvlGpUqVsllfo0YNOTs725y7SUlJOn78OOcu7quGDRtq3759SkxMtC41a9ZU27ZtrX/nXEV+EB4enuXViwcPHlTJkiUlSaVKlVJAQIDNuZqSkqLvvvuOcxX33eXLl+XgYBsrHB0dlZmZKYnzFflTTs7L2rVr6+LFi9q1a5e1zzfffKPMzEzVqlUrx/vi8vIHTGxsrDp27KiaNWsqLCxM77//vtLS0tS5c2d7l4Z/sR49emjevHlatmyZvLy8rPe4+Pj4yN3dXT4+PoqJiVFsbKz8/Pzk7e2tXr16qXbt2nrsscfsXD3+Tby8vKzPGrjJw8NDhQoVsrZzriI/6Nevn+rUqaPRo0erTZs22r59u2bNmqVZs2ZJkvX98iNHjlTZsmVVqlQpvf322woMDFTz5s3tWzz+dZo1a6ZRo0apRIkSqly5svbs2aOJEycqOjpaEucr7OfSpUs6fPiw9fPRo0eVmJgoPz8/lShR4o7nZcWKFRUZGakuXbpoxowZSk9PV8+ePfXiiy8qMDAw54Xc87PXcd99+OGHRokSJQwXFxcjLCzM2LZtm71Lwr+cpGyX+Ph4a58rV64Y3bt3NwoWLGgUKFDAaNGihXHmzBn7FQ38f399ZZhhcK4i//jiiy+MKlWqGK6urkaFChWMWbNm2azPzMw03n77baNo0aKGq6ur0bBhQyMpKclO1eLfLCUlxejTp49RokQJw83NzShdurQxePBg49q1a9Y+nK+wh3Xr1mX7b9SOHTsahpGz8/LcuXNGVFSU4enpaXh7exudO3c2UlNT76oOi2EYRl78FgEAAAAAANjinm4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAA8EBLSkpSQECAUlNTcz3GTz/9pIceekhpaWl5WBkAAIRuAAAeSFu3bpWjo6OaNGli71LsbtCgQerVq5e8vLwkSceOHVPdunXl4eGhunXr6tixYzb9mzZtqsWLF9u0VapUSY899pgmTpx4v8oGAPxLELoBAHgAxcXFqVevXtq4caNOnz5t11quX79ut30fP35cK1asUKdOnaxtr732mooXL67ExEQVK1ZM/fv3t6779NNP5eDgoFatWmUZq3Pnzpo+fbpu3LhxP0oHAPxLELoBAHjAXLp0SZ9++qleffVVNWnSRAkJCVn6fPHFF3r00Ufl5uamwoULq0WLFtZ1165d08CBAxUUFCRXV1eFhIQoLi5OkpSQkCBfX1+bsZYuXSqLxWL9PGzYMD388MP66KOPVKpUKbm5uUmSvvzySz3++OPy9fVVoUKF1LRpU/388882Y508eVJRUVHy8/OTh4eHatasqe+++07Hjh2Tg4ODdu7cadP//fffV8mSJZWZmZntd7Fw4UJVr15dxYsXt7bt379fHTt2VNmyZdWpUyft379fknTx4kW99dZbmjp1arZjPfXUUzp//rw2bNiQ7XoAAHKD0A0AwANm4cKFqlChgsqXL6927dppzpw5MgzDuv6///2vWrRoocaNG2vPnj1au3atwsLCrOs7dOig+fPna/Lkydq/f79mzpwpT0/Pu6rh8OHDWrx4sZYsWaLExERJUlpammJjY7Vz506tXbtWDg4OatGihTUwX7p0SfXq1dOpU6e0fPlyff/993r99deVmZmp4OBgRUREKD4+3mY/8fHx6tSpkxwcsv8ny6ZNm1SzZk2bturVq+vrr79WZmamVq9erWrVqkmSBgwYoB49eigoKCjbsVxcXPTwww9r06ZNd/VdAABwO072LgAAANyduLg4tWvXTpIUGRmp5ORkbdiwQfXr15ckjRo1Si+++KLeeecd6zbVq1eXJB08eFALFy7UmjVrFBERIUkqXbr0Xddw/fp1ffLJJ/L397e2/f2S7Tlz5sjf318//fSTqlSponnz5un333/Xjh075OfnJ0kKCQmx9n/55ZfVrVs3TZw4Ua6urtq9e7f27dunZcuW3bKOX375JUvonjBhgl555RUFBwerWrVqmjlzpjZu3KjExESNGzdObdq00c6dO/X0009r8uTJcnFxsW4bGBioX3755a6/DwAAboWZbgAAHiBJSUnavn27oqKiJElOTk564YUXrJeHS1JiYqIaNmyY7faJiYlydHRUvXr17qmOkiVL2gRuSTp06JCioqJUunRpeXt7Kzg4WNKf913f3HdoaKg1cP9d8+bN5ejoqM8//1zSn5e6N2jQwDpOdq5cuWK9vP2m4sWLa8WKFdb7vQsXLqzu3btrxowZGjlypLy8vJSUlKRDhw5p5syZNtu6u7vr8uXLd/NVAABwW4RuAAAeIHFxcbpx44YCAwPl5OQkJycnTZ8+XYsXL1ZycrKkP4PjrdxunSQ5ODjYXKouSenp6Vn6eXh4ZGlr1qyZzp8/r9mzZ+u7777Td999J+l/D1q7075dXFzUoUMHxcfH6/r165o3b56io6Nvu03hwoV14cKF2/YZPXq0nn76adWoUUPr169Xq1at5OzsrJYtW2r9+vU2fc+fP5/llwkAANwLQjcAAA+IGzdu6JNPPtF7772nxMRE6/L9998rMDBQ8+fPlyRVq1ZNa9euzXaMqlWrKjMz85YPC/P391dqaqrN+6pv3rN9O+fOnVNSUpLeeustNWzYUBUrVswShqtVq6bExESdP3/+luO8/PLL+vrrrzVt2jTduHFDLVu2vO1+Q0ND9dNPP91y/f79+zVv3jyNGDFCkpSRkWH9JUJ6eroyMjJs+v/www8KDQ297T4BALgbhG4AAB4QK1as0IULFxQTE6MqVarYLK1atbJeYj506FDNnz9fQ4cO1f79+7Vv3z6NGzdOkhQcHKyOHTsqOjpaS5cu1dGjR7V+/XotXLhQklSrVi0VKFBAb775pn7++WfNmzcv26ej/13BggVVqFAhzZo1S4cPH9Y333yj2NhYmz5RUVEKCAhQ8+bNtXnzZh05ckSLFy/W1q1brX0qVqyoxx57TAMHDlRUVNQdZ8efeeYZbd26NUt4liTDMNS1a1dNmjTJOjMfHh6u2bNna//+/frkk08UHh5u7X/s2DGdOnXKeq87AAB5gdANAMADIi4uThEREfLx8cmyrlWrVtq5c6f27t2r+vXra9GiRVq+fLkefvhhPfnkk9q+fbu17/Tp0/X888+re/fuqlChgrp06WKd2fbz89N//vMfrVy5UlWrVtX8+fM1bNiwO9bm4OCgBQsWaNeuXapSpYr69eund99916aPi4uLVq9erSJFiqhx48aqWrWqxo4dK0dHR5t+MTExun79+h0vLZekRo0aycnJSV9//XWWdbNmzVLRokXVtGlTa9uwYcN09epV1apVSyEhIerRo4d13fz58/X000+rZMmSd9wvAAA5ZTH+fuMWAACAHY0YMUKLFi3S3r17c9R/6tSpWr58ub766qtc7/P69esqW7as5s2bZzP7DQDAveKVYQAAIF+4dOmSjh07pilTpmjkyJE53u6VV17RxYsXlZqaKi8vr1zt+/jx43rzzTcJ3ACAPMdMNwAAyBc6deqk+fPnq3nz5po3b16Wy84BAHgQEboBAAAAADAJD1IDAAAAAMAkhG4AAAAAAExC6AYAAAAAwCSEbgAAAAAATELoBgAAAADAJIRuAAAAAABMQugGAAAAAMAkhG4AAAAAAExC6AYAAAAAwCT/D9PhjwIzgk5RAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BINARY CLASS"
      ],
      "metadata": {
        "id": "mpDbnYYrFao9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Load datasets\n",
        "info = pd.read_csv('/content/drive/MyDrive/Oulad/studentInfo.csv')\n",
        "assessment = pd.read_csv('/content/drive/MyDrive/Oulad/studentAssessment.csv')\n",
        "vle_full = pd.read_csv('/content/drive/MyDrive/Oulad/studentVle.csv')  # full vle\n",
        "\n",
        "# Sample 500 unique students from the full VLE dataset\n",
        "sampled_students = vle_full['id_student'].drop_duplicates().sample(n=6000, random_state=42)\n",
        "\n",
        "# Filter the full vle data for only these sampled students\n",
        "vle = vle_full[vle_full['id_student'].isin(sampled_students)]\n",
        "\n",
        "# Feature Engineering\n",
        "\n",
        "avg_scores = assessment.groupby(\"id_student\")[\"score\"].mean().reset_index()\n",
        "avg_scores.columns = [\"id_student\", \"avg_score\"]\n",
        "\n",
        "num_assessments = assessment.groupby(\"id_student\")[\"id_assessment\"].count().reset_index()\n",
        "num_assessments.columns = [\"id_student\", \"num_assessments\"]\n",
        "\n",
        "clicks = vle.groupby(\"id_student\")[\"sum_click\"].agg(['sum', 'count']).reset_index()\n",
        "clicks.columns = [\"id_student\", \"total_clicks\", \"num_sessions\"]\n",
        "\n",
        "df = info.merge(avg_scores, on=\"id_student\", how=\"left\")\n",
        "df = df.merge(num_assessments, on=\"id_student\", how=\"left\")\n",
        "df = df.merge(clicks, on=\"id_student\", how=\"left\")\n",
        "\n",
        "# Keep only rows for sampled students (since info is full data)\n",
        "df = df[df[\"id_student\"].isin(sampled_students)]\n",
        "\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Derived features\n",
        "df[\"clicks_per_session\"] = df[\"total_clicks\"] / (df[\"num_sessions\"] + 1e-6)\n",
        "df[\"score_per_assessment\"] = df[\"avg_score\"] / (df[\"num_assessments\"] + 1e-6)\n",
        "df[\"clicks_per_credit\"] = df[\"total_clicks\"] / (df[\"studied_credits\"] + 1e-6)\n",
        "\n",
        "# Performance band (binary classification: High vs Low)\n",
        "def band(result):\n",
        "    if result == \"Distinction\":\n",
        "        return \"High\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "df[\"performance_band\"] = df[\"final_result\"].apply(band)\n",
        "\n",
        "# Encode categorical features\n",
        "cat_cols = [\"gender\", \"age_band\", \"highest_education\", \"imd_band\", \"disability\"]\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df[col] = df[col].astype(str)\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Final feature list\n",
        "features = cat_cols + [\n",
        "    \"studied_credits\", \"num_of_prev_attempts\",\n",
        "    \"avg_score\", \"total_clicks\", \"num_sessions\", \"num_assessments\",\n",
        "    \"clicks_per_session\", \"score_per_assessment\", \"clicks_per_credit\"\n",
        "]\n",
        "\n",
        "X = df[features]\n",
        "y = df[\"performance_band\"]\n",
        "target_le = LabelEncoder()\n",
        "y_encoded = target_le.fit_transform(y)\n",
        "\n",
        "# Time-based train/test split using id_student rank ordering\n",
        "df[\"id_order\"] = df[\"id_student\"].rank(method=\"first\")\n",
        "split_index = int(len(df) * 0.7)\n",
        "\n",
        "X_train = X[df[\"id_order\"] <= split_index]\n",
        "X_test = X[df[\"id_order\"] > split_index]\n",
        "y_train = y_encoded[df[\"id_order\"] <= split_index]\n",
        "y_test = y_encoded[df[\"id_order\"] > split_index]\n",
        "\n",
        "# Handle class imbalance by assigning class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(y_train),\n",
        "    y=y_train\n",
        ")\n",
        "class_weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "\n",
        "# XGBoost model for binary classification with class weights\n",
        "model = XGBClassifier(\n",
        "    objective=\"binary:logistic\",\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    n_estimators=300,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    eval_metric='logloss',\n",
        "    scale_pos_weight=class_weights_dict.get(0, 1)  # Use get to avoid key errors\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_test_label = target_le.inverse_transform(y_test)\n",
        "y_pred_label = target_le.inverse_transform(y_pred)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Accuracy:\", accuracy_score(y_test_label, y_pred_label))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_label, y_pred_label))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test_label, y_pred_label))\n",
        "\n",
        "# # Uncomment for SHAP explainability (needs shap installed)\n",
        "# explainer = shap.Explainer(model, X_train)\n",
        "# shap_values = explainer(X_test)\n",
        "# shap.summary_plot(shap_values, X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOq5OdWEyhqz",
        "outputId": "0d1c302e-b419-495c-af68-762dec2e9133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9050632911392406\n",
            "Confusion Matrix:\n",
            " [[  66  171]\n",
            " [  24 1793]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        High       0.73      0.28      0.40       237\n",
            "         Low       0.91      0.99      0.95      1817\n",
            "\n",
            "    accuracy                           0.91      2054\n",
            "   macro avg       0.82      0.63      0.68      2054\n",
            "weighted avg       0.89      0.91      0.89      2054\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Initialize models (add hyperparameters as needed)\n",
        "xgb_model = XGBClassifier(objective=\"binary:logistic\", use_label_encoder=False, eval_metric='logloss')\n",
        "lgbm_model = LGBMClassifier(objective='binary')\n",
        "rf_model = RandomForestClassifier()\n",
        "lr_model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Fit models\n",
        "xgb_model.fit(X_train, y_train)\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "rf_model.fit(X_train, y_train)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities for the positive class (class 1)\n",
        "xgb_probs = xgb_model.predict_proba(X_test)[:, 1]\n",
        "lgbm_probs = lgbm_model.predict_proba(X_test)[:, 1]\n",
        "rf_probs = rf_model.predict_proba(X_test)[:, 1]\n",
        "lr_probs = lr_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Define equal weights for soft voting (can be tuned)\n",
        "weights = [0.25, 0.25, 0.25, 0.25]\n",
        "\n",
        "# Compute weighted average of predicted probabilities\n",
        "final_probs = (\n",
        "    weights[3] * xgb_probs +\n",
        "    weights[1] * lgbm_probs +\n",
        "    weights[1] * rf_probs +\n",
        "    weights[1] * lr_probs\n",
        ")\n",
        "\n",
        "# Convert probabilities to binary predictions with threshold 0.5\n",
        "final_preds = (final_probs > 0.5).astype(int)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Soft Voting Ensemble Accuracy:\", accuracy_score(y_test, final_preds))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, final_preds))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAWPPR_a0BhW",
        "outputId": "e5415c72-f273-4c19-c3ec-351bfd90e409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:29:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 4367, number of negative: 423\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1605\n",
            "[LightGBM] [Info] Number of data points in the train set: 4790, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.911691 -> initscore=2.334459\n",
            "[LightGBM] [Info] Start training from score 2.334459\n",
            "Soft Voting Ensemble Accuracy: 0.9162609542356378\n",
            "Confusion Matrix:\n",
            " [[  92  145]\n",
            " [  27 1790]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.39      0.52       237\n",
            "           1       0.93      0.99      0.95      1817\n",
            "\n",
            "    accuracy                           0.92      2054\n",
            "   macro avg       0.85      0.69      0.74      2054\n",
            "weighted avg       0.91      0.92      0.90      2054\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI CLASS"
      ],
      "metadata": {
        "id": "M89oN5OrFf93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from xgboost import XGBClassifier\n",
        "import re\n",
        "\n",
        "# Load data\n",
        "info = pd.read_csv('/content/drive/MyDrive/Oulad/studentInfo.csv')\n",
        "assessment = pd.read_csv('/content/drive/MyDrive/Oulad/studentAssessment.csv')\n",
        "vle_full = pd.read_csv('/content/drive/MyDrive/Oulad/studentVle.csv')\n",
        "\n",
        "# Sample 5000 students\n",
        "sampled_students = vle_full['id_student'].drop_duplicates().sample(n=3000, random_state=42)\n",
        "vle = vle_full[vle_full['id_student'].isin(sampled_students)]\n",
        "\n",
        "# Merge VLE date info to get the last activity date per student\n",
        "vle_dates = vle.groupby(\"id_student\")[\"date\"].max().reset_index()\n",
        "vle_dates.columns = [\"id_student\", \"last_vle_date\"]\n",
        "\n",
        "# Aggregate assessment and VLE data\n",
        "avg_scores = assessment.groupby(\"id_student\")[\"score\"].mean().reset_index()\n",
        "avg_scores.columns = [\"id_student\", \"avg_score\"]\n",
        "\n",
        "num_assessments = assessment.groupby(\"id_student\")[\"id_assessment\"].count().reset_index()\n",
        "num_assessments.columns = [\"id_student\", \"num_assessments\"]\n",
        "\n",
        "std_scores = assessment.groupby(\"id_student\")[\"score\"].std().reset_index().fillna(0)\n",
        "std_scores.columns = [\"id_student\", \"score_std\"]\n",
        "\n",
        "clicks = vle.groupby(\"id_student\")[\"sum_click\"].agg(['sum', 'count', 'mean', 'std']).reset_index().fillna(0)\n",
        "clicks.columns = [\"id_student\", \"total_clicks\", \"num_sessions\", \"avg_clicks\", \"std_clicks\"]\n",
        "\n",
        "df = info.merge(avg_scores, on=\"id_student\", how=\"left\")\n",
        "df = df.merge(num_assessments, on=\"id_student\", how=\"left\")\n",
        "df = df.merge(std_scores, on=\"id_student\", how=\"left\")\n",
        "df = df.merge(clicks, on=\"id_student\", how=\"left\")\n",
        "df = df.merge(vle_dates, on=\"id_student\", how=\"left\")\n",
        "\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "# Derived features\n",
        "df[\"clicks_per_session\"] = df[\"total_clicks\"] / (df[\"num_sessions\"] + 1e-6)\n",
        "df[\"score_per_assessment\"] = df[\"avg_score\"] / (df[\"num_assessments\"] + 1e-6)\n",
        "df[\"clicks_per_credit\"] = df[\"total_clicks\"] / (df[\"studied_credits\"] + 1e-6)\n",
        "df[\"has_clicks\"] = (df[\"total_clicks\"] > 0).astype(int)\n",
        "df[\"has_assessments\"] = (df[\"num_assessments\"] > 0).astype(int)\n",
        "\n",
        "# Label target\n",
        "def band_multiclass(result):\n",
        "    if result == \"Distinction\":\n",
        "        return \"High\"\n",
        "    elif result == \"Pass\":\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "df[\"performance_band\"] = df[\"final_result\"].apply(band_multiclass)\n",
        "\n",
        "# Filter only active students\n",
        "df = df[(df[\"has_clicks\"] == 1) | (df[\"has_assessments\"] == 1)]\n",
        "\n",
        "# One-hot encode\n",
        "categorical_cols = [\"gender\", \"age_band\", \"highest_education\", \"imd_band\", \"disability\", \"code_module\", \"code_presentation\", \"region\"]\n",
        "df[categorical_cols] = df[categorical_cols].astype(str)\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Prepare final dataset\n",
        "drop_cols = [\"id_student\", \"final_result\", \"performance_band\"]\n",
        "X = df.drop(columns=drop_cols)\n",
        "y = df[\"performance_band\"]\n",
        "y_encoded, class_names = pd.factorize(y)\n",
        "\n",
        "# Clean and scale\n",
        "X.columns = [re.sub(r'[\\[\\]<>]', '_', \"\".join(c if c.isalnum() else \"_\" for c in col)) for col in X.columns]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "# Time-based split based on last VLE activity\n",
        "df[\"y_encoded\"] = y_encoded\n",
        "df_sorted = df.sort_values(by=\"last_vle_date\")\n",
        "split_idx = int(len(df_sorted) * 0.7)\n",
        "train_ids = df_sorted.iloc[:split_idx][\"id_student\"]\n",
        "test_ids = df_sorted.iloc[split_idx:][\"id_student\"]\n",
        "\n",
        "train_mask = df[\"id_student\"].isin(train_ids).values\n",
        "test_mask = df[\"id_student\"].isin(test_ids).values\n",
        "\n",
        "X_train = X_scaled.loc[train_mask].reset_index(drop=True)\n",
        "y_train = df.loc[train_mask, \"y_encoded\"].reset_index(drop=True)\n",
        "\n",
        "X_test = X_scaled.loc[test_mask].reset_index(drop=True)\n",
        "y_test = df.loc[test_mask, \"y_encoded\"].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Sample weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
        "sample_weights = np.array([weights_dict[cls] for cls in y_train])\n",
        "\n",
        "# Train XGBoost\n",
        "model = XGBClassifier(\n",
        "    objective=\"multi:softprob\",\n",
        "    num_class=3,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=10,\n",
        "    min_child_weight=2,\n",
        "    gamma=0.15,\n",
        "    subsample=0.85,\n",
        "    colsample_bytree=0.85,\n",
        "    reg_alpha=0.2,\n",
        "    reg_lambda=1.2,\n",
        "    n_estimators=800,\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    tree_method=\"hist\",\n",
        "    verbosity=0\n",
        ")\n",
        "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=class_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MsqCrXA0wLw",
        "outputId": "563bd4cd-72f4-4b15-c552-c1c451cef860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 83.76 %\n",
            "Confusion Matrix:\n",
            " [[3008  730  271]\n",
            " [ 348 4477   37]\n",
            " [ 167   65  858]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "      Medium       0.85      0.75      0.80      4009\n",
            "         Low       0.85      0.92      0.88      4862\n",
            "        High       0.74      0.79      0.76      1090\n",
            "\n",
            "    accuracy                           0.84      9961\n",
            "   macro avg       0.81      0.82      0.81      9961\n",
            "weighted avg       0.84      0.84      0.84      9961\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENGAGEMENT LEVEL PREDICTION"
      ],
      "metadata": {
        "id": "PFH2HxwQFtMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Bidirectional, LSTM, Dense, Dropout, Attention\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Lambda\n",
        "\n",
        "# --- Load data ---\n",
        "vle = pd.read_csv(\"/content/drive/MyDrive/Oulad/studentVle.csv\")\n",
        "info = pd.read_csv(\"/content/drive/MyDrive/Oulad/studentInfo.csv\")\n",
        "\n",
        "# --- Preprocess ---\n",
        "vle['date'] = pd.to_numeric(vle['date'], errors='coerce')\n",
        "vle['week'] = (vle['date'] // 7).astype(int)\n",
        "\n",
        "# --- Aggregate weekly clicks ---\n",
        "weekly_clicks = vle.groupby(['id_student', 'week'])['sum_click'].sum().unstack(fill_value=0)\n",
        "weekly_clicks = weekly_clicks.reindex(columns=range(0, 54), fill_value=0)\n",
        "weekly_clicks = weekly_clicks.div(weekly_clicks.max(axis=1).replace(0, 1), axis=0)\n",
        "\n",
        "# --- Label engagement levels from total_clicks ---\n",
        "total_clicks = weekly_clicks.sum(axis=1)\n",
        "engagement_level = pd.qcut(total_clicks, q=3, labels=['Low', 'Moderate', 'High'])\n",
        "labels_df = pd.DataFrame({'id_student': total_clicks.index, 'engagement_level': engagement_level})\n",
        "\n",
        "# --- Align inputs and labels ---\n",
        "weekly_clicks = weekly_clicks.loc[weekly_clicks.index.isin(labels_df['id_student'])]\n",
        "labels_df = labels_df.set_index('id_student').loc[weekly_clicks.index]\n",
        "\n",
        "# --- Prepare data for model ---\n",
        "X = weekly_clicks.values[..., np.newaxis]\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(labels_df['engagement_level'])\n",
        "y_categorical = to_categorical(y_encoded)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42, stratify=y_categorical)\n",
        "\n",
        "# --- Define model ---\n",
        "input_layer = Input(shape=(X.shape[1], 1))\n",
        "x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(input_layer)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "x = Attention(use_scale=True)([x, x])\n",
        "# Replace tf.reduce_mean with Lambda layer\n",
        "x = Lambda(lambda t: tf.reduce_mean(t, axis=1))(x)\n",
        "x = Dropout(0.4)(x)\n",
        "output_layer = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# --- Callbacks ---\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, verbose=1)\n",
        "\n",
        "# --- Train ---\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.2,\n",
        "    epochs=20,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stop, reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Evaluate ---\n",
        "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {acc * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCRDJUEe2OkF",
        "outputId": "3ed22888-5776-4f2f-8d72-3c47c852ff00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 132ms/step - accuracy: 0.7034 - loss: 0.6121 - val_accuracy: 0.9204 - val_loss: 0.2015 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 120ms/step - accuracy: 0.9253 - loss: 0.1943 - val_accuracy: 0.9569 - val_loss: 0.1148 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 125ms/step - accuracy: 0.9330 - loss: 0.1661 - val_accuracy: 0.9092 - val_loss: 0.1780 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 119ms/step - accuracy: 0.9425 - loss: 0.1424 - val_accuracy: 0.9463 - val_loss: 0.1246 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.9502 - loss: 0.1216\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 134ms/step - accuracy: 0.9502 - loss: 0.1216 - val_accuracy: 0.9367 - val_loss: 0.1399 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 116ms/step - accuracy: 0.9605 - loss: 0.1029 - val_accuracy: 0.9672 - val_loss: 0.0770 - learning_rate: 5.0000e-04\n",
            "Epoch 7/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 119ms/step - accuracy: 0.9574 - loss: 0.1017 - val_accuracy: 0.9643 - val_loss: 0.0852 - learning_rate: 5.0000e-04\n",
            "Epoch 8/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 120ms/step - accuracy: 0.9578 - loss: 0.1009 - val_accuracy: 0.9784 - val_loss: 0.0585 - learning_rate: 5.0000e-04\n",
            "Epoch 9/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 119ms/step - accuracy: 0.9568 - loss: 0.1041 - val_accuracy: 0.9739 - val_loss: 0.0725 - learning_rate: 5.0000e-04\n",
            "Epoch 10/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 116ms/step - accuracy: 0.9614 - loss: 0.0913 - val_accuracy: 0.9501 - val_loss: 0.1226 - learning_rate: 5.0000e-04\n",
            "Epoch 11/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 120ms/step - accuracy: 0.9587 - loss: 0.0977 - val_accuracy: 0.9818 - val_loss: 0.0550 - learning_rate: 5.0000e-04\n",
            "Epoch 12/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 119ms/step - accuracy: 0.9628 - loss: 0.0901 - val_accuracy: 0.9607 - val_loss: 0.0815 - learning_rate: 5.0000e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 115ms/step - accuracy: 0.9608 - loss: 0.0952 - val_accuracy: 0.9813 - val_loss: 0.0532 - learning_rate: 5.0000e-04\n",
            "Epoch 14/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 115ms/step - accuracy: 0.9664 - loss: 0.0844 - val_accuracy: 0.9813 - val_loss: 0.0518 - learning_rate: 5.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - accuracy: 0.9596 - loss: 0.0920 - val_accuracy: 0.9715 - val_loss: 0.0719 - learning_rate: 5.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 116ms/step - accuracy: 0.9611 - loss: 0.0920 - val_accuracy: 0.9803 - val_loss: 0.0653 - learning_rate: 5.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9642 - loss: 0.0856\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - accuracy: 0.9642 - loss: 0.0856 - val_accuracy: 0.9791 - val_loss: 0.0543 - learning_rate: 5.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 115ms/step - accuracy: 0.9703 - loss: 0.0727 - val_accuracy: 0.9779 - val_loss: 0.0561 - learning_rate: 2.5000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 116ms/step - accuracy: 0.9679 - loss: 0.0767 - val_accuracy: 0.9710 - val_loss: 0.0617 - learning_rate: 2.5000e-04\n",
            "\n",
            "Test Accuracy: 98.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ENGAGEMENT PREDICTION: CNN + BiLSTM + Attention model\n",
        "PERFORMANCE PREDICTION: StackingClassifier (XGBoost, LightGBM, Random Forest, Logistic Regression)"
      ],
      "metadata": {
        "id": "-jxSjLlVFzE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Bidirectional, LSTM, Dense, Dropout, Lambda, Attention\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- Load Data ---\n",
        "vle = pd.read_csv(\"/content/drive/MyDrive/Oulad/studentVle.csv\")\n",
        "info = pd.read_csv(\"/content/drive/MyDrive/Oulad/studentInfo.csv\")\n",
        "assessment = pd.read_csv(\"/content/drive/MyDrive/Oulad/studentAssessment.csv\")\n",
        "\n",
        "# --- Engagement Data Preprocessing ---\n",
        "vle['date'] = pd.to_numeric(vle['date'], errors='coerce')\n",
        "vle['week'] = (vle['date'] // 7).astype(int)\n",
        "weekly_clicks = vle.groupby(['id_student', 'week'])['sum_click'].sum().unstack(fill_value=0)\n",
        "weekly_clicks = weekly_clicks.reindex(columns=range(0, 54), fill_value=0)\n",
        "weekly_clicks = weekly_clicks.div(weekly_clicks.max(axis=1).replace(0, 1), axis=0)\n",
        "total_clicks = weekly_clicks.sum(axis=1)\n",
        "engagement_level = pd.qcut(total_clicks, q=3, labels=['Low', 'Moderate', 'High'])\n",
        "labels_df = pd.DataFrame({'id_student': total_clicks.index, 'engagement_level': engagement_level})\n",
        "\n",
        "# --- Engagement Model Inputs ---\n",
        "X_engage = weekly_clicks.values[..., np.newaxis]\n",
        "le_engage = LabelEncoder()\n",
        "y_engage = to_categorical(le_engage.fit_transform(labels_df['engagement_level']))\n",
        "X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(\n",
        "    X_engage, y_engage, test_size=0.2, random_state=42, stratify=y_engage)\n",
        "\n",
        "# --- Engagement Model (CNN + BiLSTM + Attention) ---\n",
        "input_layer = Input(shape=(X_engage.shape[1], 1))\n",
        "x = Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "x = Attention(use_scale=True)([x, x])\n",
        "x = Lambda(lambda t: tf.reduce_mean(t, axis=1))(x)\n",
        "x = Dropout(0.4)(x)\n",
        "out_layer = Dense(3, activation='softmax')(x)\n",
        "model = Model(inputs=input_layer, outputs=out_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_eng_train, y_eng_train, validation_split=0.2, epochs=20, batch_size=64,\n",
        "          callbacks=[EarlyStopping(patience=5, restore_best_weights=True), ReduceLROnPlateau(patience=3)], verbose=1)\n",
        "\n",
        "# --- Engagement Predictions ---\n",
        "engage_preds = model.predict(X_engage, verbose=0)\n",
        "engage_labels = le_engage.inverse_transform(np.argmax(engage_preds, axis=1))\n",
        "engage_df = pd.DataFrame({'id_student': weekly_clicks.index, 'engagement_pred': engage_labels})\n",
        "engage_df[['engage_prob_low', 'engage_prob_moderate', 'engage_prob_high']] = engage_preds\n",
        "\n",
        "# --- Feature Engineering for Performance Prediction ---\n",
        "sampled_students = engage_df['id_student']\n",
        "vle = vle[vle['id_student'].isin(sampled_students)]\n",
        "avg_scores = assessment.groupby(\"id_student\")[\"score\"].mean().reset_index()\n",
        "avg_scores.columns = [\"id_student\", \"avg_score\"]\n",
        "num_assessments = assessment.groupby(\"id_student\")[\"id_assessment\"].count().reset_index()\n",
        "num_assessments.columns = [\"id_student\", \"num_assessments\"]\n",
        "clicks = vle.groupby(\"id_student\")[\"sum_click\"].agg(['sum', 'count']).reset_index()\n",
        "clicks.columns = [\"id_student\", \"total_clicks\", \"num_sessions\"]\n",
        "\n",
        "perf_df = info.merge(avg_scores, on=\"id_student\", how=\"left\")\n",
        "perf_df = perf_df.merge(num_assessments, on=\"id_student\", how=\"left\")\n",
        "perf_df = perf_df.merge(clicks, on=\"id_student\", how=\"left\")\n",
        "perf_df = perf_df.merge(engage_df, on=\"id_student\", how=\"left\")\n",
        "perf_df.fillna(0, inplace=True)\n",
        "\n",
        "# --- Derived Features ---\n",
        "perf_df[\"clicks_per_session\"] = perf_df[\"total_clicks\"] / (perf_df[\"num_sessions\"] + 1e-6)\n",
        "perf_df[\"score_per_assessment\"] = perf_df[\"avg_score\"] / (perf_df[\"num_assessments\"] + 1e-6)\n",
        "perf_df[\"clicks_per_credit\"] = perf_df[\"total_clicks\"] / (perf_df[\"studied_credits\"] + 1e-6)\n",
        "\n",
        "# --- Add Raw Weekly Clicks ---\n",
        "weekly_df = pd.DataFrame(weekly_clicks, index=weekly_clicks.index)\n",
        "weekly_df.columns = [f\"week_{i}\" for i in range(54)]\n",
        "perf_df = perf_df.set_index(\"id_student\").join(weekly_df).reset_index()\n",
        "perf_df[\"early_clicks\"] = perf_df[[f\"week_{i}\" for i in range(4)]].sum(axis=1)\n",
        "perf_df[\"late_clicks\"] = perf_df[[f\"week_{i}\" for i in range(50, 54)]].sum(axis=1)\n",
        "\n",
        "# --- Target Variable ---\n",
        "perf_df[\"performance_band\"] = perf_df[\"final_result\"].apply(lambda x: \"High\" if x == \"Distinction\" else \"Low\")\n",
        "\n",
        "# --- Encode Features ---\n",
        "cat_cols = [\"gender\", \"age_band\", \"highest_education\", \"imd_band\", \"disability\", \"engagement_pred\"]\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    perf_df[col] = le.fit_transform(perf_df[col].astype(str))\n",
        "\n",
        "# --- Final Feature Set ---\n",
        "features = cat_cols + [\n",
        "    \"studied_credits\", \"num_of_prev_attempts\", \"avg_score\",\n",
        "    \"total_clicks\", \"num_sessions\", \"num_assessments\",\n",
        "    \"clicks_per_session\", \"score_per_assessment\", \"clicks_per_credit\",\n",
        "    \"early_clicks\", \"late_clicks\",\n",
        "    \"engage_prob_low\", \"engage_prob_moderate\", \"engage_prob_high\"\n",
        "]\n",
        "X = perf_df[features]\n",
        "y = LabelEncoder().fit_transform(perf_df[\"performance_band\"])\n",
        "\n",
        "# --- Train/Test Split ---\n",
        "perf_df[\"id_order\"] = perf_df[\"id_student\"].rank(method=\"first\")\n",
        "split = int(len(perf_df) * 0.7)\n",
        "X_train = X[perf_df[\"id_order\"] <= split]\n",
        "X_test = X[perf_df[\"id_order\"] > split]\n",
        "y_train = y[perf_df[\"id_order\"] <= split]\n",
        "y_test = y[perf_df[\"id_order\"] > split]\n",
        "\n",
        "# --- Class Weights ---\n",
        "cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "cw_dict = dict(zip(np.unique(y_train), cw))\n",
        "\n",
        "# --- Models ---\n",
        "xgb = XGBClassifier(scale_pos_weight=cw_dict.get(0, 1), eval_metric='logloss', use_label_encoder=False)\n",
        "lgbm = LGBMClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "lr = LogisticRegression(max_iter=500)\n",
        "\n",
        "# --- Stacking ---\n",
        "stack = StackingClassifier(\n",
        "    estimators=[('xgb', xgb), ('lgbm', lgbm), ('rf', rf), ('lr', lr)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=True\n",
        ")\n",
        "stack.fit(X_train, y_train)\n",
        "final_preds = stack.predict(X_test)\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nFinal Stacked Accuracy:\", accuracy_score(y_test, final_preds))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, final_preds))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5oX97eD29Qh",
        "outputId": "19ec9be5-0598-4c62-e8b5-b0c50b427465"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 98ms/step - accuracy: 0.7114 - loss: 0.5692 - val_accuracy: 0.8888 - val_loss: 0.2447 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 96ms/step - accuracy: 0.9349 - loss: 0.1688 - val_accuracy: 0.9135 - val_loss: 0.1882 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step - accuracy: 0.9436 - loss: 0.1443 - val_accuracy: 0.9223 - val_loss: 0.1761 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 101ms/step - accuracy: 0.9534 - loss: 0.1182 - val_accuracy: 0.9636 - val_loss: 0.0926 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 97ms/step - accuracy: 0.9495 - loss: 0.1235 - val_accuracy: 0.9578 - val_loss: 0.0981 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 94ms/step - accuracy: 0.9548 - loss: 0.1095 - val_accuracy: 0.9401 - val_loss: 0.1561 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 101ms/step - accuracy: 0.9509 - loss: 0.1215 - val_accuracy: 0.9727 - val_loss: 0.0729 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 106ms/step - accuracy: 0.9561 - loss: 0.1007 - val_accuracy: 0.9513 - val_loss: 0.1012 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 103ms/step - accuracy: 0.9600 - loss: 0.1010 - val_accuracy: 0.9669 - val_loss: 0.0873 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 100ms/step - accuracy: 0.9608 - loss: 0.0992 - val_accuracy: 0.9753 - val_loss: 0.0652 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - accuracy: 0.9617 - loss: 0.0933 - val_accuracy: 0.9660 - val_loss: 0.0799 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 99ms/step - accuracy: 0.9597 - loss: 0.0917 - val_accuracy: 0.9705 - val_loss: 0.0684 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - accuracy: 0.9584 - loss: 0.0916 - val_accuracy: 0.9688 - val_loss: 0.0678 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 99ms/step - accuracy: 0.9684 - loss: 0.0718 - val_accuracy: 0.9734 - val_loss: 0.0598 - learning_rate: 1.0000e-04\n",
            "Epoch 15/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 104ms/step - accuracy: 0.9740 - loss: 0.0641 - val_accuracy: 0.9765 - val_loss: 0.0554 - learning_rate: 1.0000e-04\n",
            "Epoch 16/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 99ms/step - accuracy: 0.9759 - loss: 0.0614 - val_accuracy: 0.9794 - val_loss: 0.0519 - learning_rate: 1.0000e-04\n",
            "Epoch 17/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - accuracy: 0.9747 - loss: 0.0622 - val_accuracy: 0.9842 - val_loss: 0.0464 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 106ms/step - accuracy: 0.9720 - loss: 0.0653 - val_accuracy: 0.9784 - val_loss: 0.0528 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 105ms/step - accuracy: 0.9734 - loss: 0.0624 - val_accuracy: 0.9794 - val_loss: 0.0498 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 106ms/step - accuracy: 0.9758 - loss: 0.0608 - val_accuracy: 0.9827 - val_loss: 0.0492 - learning_rate: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:18:25] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 20938, number of negative: 1877\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003709 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2646\n",
            "[LightGBM] [Info] Number of data points in the train set: 22815, number of used features: 19\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.917730 -> initscore=2.411891\n",
            "[LightGBM] [Info] Start training from score 2.411891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:18:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:18:35] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:18:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:18:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:18:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 16750, number of negative: 1502\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001847 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2642\n",
            "[LightGBM] [Info] Number of data points in the train set: 18252, number of used features: 19\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.917708 -> initscore=2.411601\n",
            "[LightGBM] [Info] Start training from score 2.411601\n",
            "[LightGBM] [Info] Number of positive: 16750, number of negative: 1502\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001811 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2645\n",
            "[LightGBM] [Info] Number of data points in the train set: 18252, number of used features: 19\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.917708 -> initscore=2.411601\n",
            "[LightGBM] [Info] Start training from score 2.411601\n",
            "[LightGBM] [Info] Number of positive: 16750, number of negative: 1502\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001950 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2639\n",
            "[LightGBM] [Info] Number of data points in the train set: 18252, number of used features: 19\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.917708 -> initscore=2.411601\n",
            "[LightGBM] [Info] Start training from score 2.411601\n",
            "[LightGBM] [Info] Number of positive: 16751, number of negative: 1501\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001911 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2643\n",
            "[LightGBM] [Info] Number of data points in the train set: 18252, number of used features: 19\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.917762 -> initscore=2.412326\n",
            "[LightGBM] [Info] Start training from score 2.412326\n",
            "[LightGBM] [Info] Number of positive: 16751, number of negative: 1501\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002854 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2645\n",
            "[LightGBM] [Info] Number of data points in the train set: 18252, number of used features: 19\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.917762 -> initscore=2.412326\n",
            "[LightGBM] [Info] Start training from score 2.412326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Stacked Accuracy: 0.9110247494375128\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 395  752]\n",
            " [ 118 8513]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.34      0.48      1147\n",
            "           1       0.92      0.99      0.95      8631\n",
            "\n",
            "    accuracy                           0.91      9778\n",
            "   macro avg       0.84      0.67      0.71      9778\n",
            "weighted avg       0.90      0.91      0.90      9778\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INDIAN DATASET"
      ],
      "metadata": {
        "id": "RaRVTSfRGSua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, Bidirectional, LSTM, Dense, Dropout, Lambda, Attention\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- Load Data ---\n",
        "vle = pd.read_csv(\"/content/drive/MyDrive/indian/studentVLE_india.csv\")\n",
        "info = pd.read_csv(\"/content/drive/MyDrive/indian/studentInfo_india.csv\")\n",
        "assessment = pd.read_csv(\"/content/drive/MyDrive/indian/studentAssessment_india.csv\")"
      ],
      "metadata": {
        "id": "Dne8vEi26WDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "metadata": {
        "id": "F6fCM5ljCVH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- Preprocessing VLE Engagement Data ---\n",
        "vle = vle.rename(columns={'student_id': 'id_student', 'clicks': 'sum_click'})  # Rename to standardize\n",
        "vle['week'] = pd.to_numeric(vle['week'], errors='coerce')\n",
        "\n",
        "weekly_clicks = vle.groupby(['id_student', 'week'])['sum_click'].sum().unstack(fill_value=0)\n",
        "weekly_clicks = weekly_clicks.reindex(columns=range(0, 54), fill_value=0)  # ensure all weeks\n",
        "weekly_clicks = weekly_clicks.div(weekly_clicks.max(axis=1).replace(0, 1), axis=0)  # normalize\n",
        "\n",
        "total_clicks = weekly_clicks.sum(axis=1)\n",
        "engagement_level = pd.qcut(total_clicks, q=3, labels=['Low', 'Moderate', 'High'])\n",
        "labels_df = pd.DataFrame({'id_student': total_clicks.index, 'engagement_level': engagement_level})\n"
      ],
      "metadata": {
        "id": "gKthLOPaCYI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Engagement Model Inputs ---\n",
        "X_engage = weekly_clicks.values[..., np.newaxis]\n",
        "le_engage = LabelEncoder()\n",
        "y_engage = to_categorical(le_engage.fit_transform(labels_df['engagement_level']))\n",
        "\n",
        "X_eng_train, X_eng_test, y_eng_train, y_eng_test = train_test_split(\n",
        "    X_engage, y_engage, test_size=0.2, random_state=42, stratify=y_engage\n",
        ")\n",
        "\n",
        "# --- Engagement Model ---\n",
        "input_layer = Input(shape=(X_engage.shape[1], 1))\n",
        "x = Conv1D(64, 3, activation='relu', padding='same')(input_layer)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "x = Attention(use_scale=True)([x, x])\n",
        "x = Lambda(lambda t: tf.reduce_mean(t, axis=1))(x)\n",
        "x = Dropout(0.4)(x)\n",
        "out_layer = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer, outputs=out_layer)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(\n",
        "    X_eng_train, y_eng_train,\n",
        "    validation_split=0.2, epochs=20, batch_size=64,\n",
        "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True), ReduceLROnPlateau(patience=3)],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Engagement Predictions ---\n",
        "engage_preds = model.predict(X_engage, verbose=0)\n",
        "engage_labels = le_engage.inverse_transform(np.argmax(engage_preds, axis=1))\n",
        "engage_df = pd.DataFrame({'id_student': weekly_clicks.index, 'engagement_pred': engage_labels})\n",
        "engage_df[['engage_prob_low', 'engage_prob_moderate', 'engage_prob_high']] = engage_preds\n",
        "\n",
        "# --- Feature Engineering for Performance Prediction ---\n",
        "\n",
        "# Rename columns in assessment for consistency\n",
        "assessment = assessment.rename(columns={'student_id': 'id_student', 'assessment_id': 'id_assessment'})\n",
        "\n",
        "sampled_students = engage_df['id_student']\n",
        "vle = vle[vle['id_student'].isin(sampled_students)]\n",
        "\n",
        "avg_scores = assessment.groupby(\"id_student\")[\"score\"].mean().reset_index()\n",
        "avg_scores.columns = [\"id_student\", \"avg_score\"]\n",
        "\n",
        "num_assessments = assessment.groupby(\"id_student\")[\"id_assessment\"].count().reset_index()\n",
        "num_assessments.columns = [\"id_student\", \"num_assessments\"]\n",
        "\n",
        "clicks = vle.groupby(\"id_student\")[\"sum_click\"].agg(['sum', 'count']).reset_index()\n",
        "clicks.columns = [\"id_student\", \"total_clicks\", \"num_sessions\"]\n",
        "\n",
        "# Rename 'student_id' to 'id_student' in info for merging consistency\n",
        "info = info.rename(columns={'student_id': 'id_student'})\n",
        "\n",
        "perf_df = info.merge(avg_scores, on=\"id_student\", how=\"left\")\n",
        "perf_df = perf_df.merge(num_assessments, on=\"id_student\", how=\"left\")\n",
        "perf_df = perf_df.merge(clicks, on=\"id_student\", how=\"left\")\n",
        "perf_df = perf_df.merge(engage_df, on=\"id_student\", how=\"left\")\n",
        "\n",
        "perf_df.fillna(0, inplace=True)\n",
        "\n",
        "# --- Derived Features ---\n",
        "perf_df[\"clicks_per_session\"] = perf_df[\"total_clicks\"] / (perf_df[\"num_sessions\"] + 1e-6)\n",
        "perf_df[\"score_per_assessment\"] = perf_df[\"avg_score\"] / (perf_df[\"num_assessments\"] + 1e-6)\n",
        "perf_df[\"clicks_per_credit\"] = perf_df[\"total_clicks\"] / (perf_df[\"studied_credits\"] + 1e-6)\n",
        "\n",
        "# --- Add Raw Weekly Clicks ---\n",
        "weekly_df = pd.DataFrame(weekly_clicks, index=weekly_clicks.index)\n",
        "weekly_df.columns = [f\"week_{i}\" for i in range(54)]\n",
        "perf_df = perf_df.set_index(\"id_student\").join(weekly_df).reset_index()\n",
        "\n",
        "perf_df[\"early_clicks\"] = perf_df[[f\"week_{i}\" for i in range(4)]].sum(axis=1)\n",
        "perf_df[\"late_clicks\"] = perf_df[[f\"week_{i}\" for i in range(50, 54)]].sum(axis=1)\n",
        "\n",
        "# --- Target Variable ---\n",
        "# Create target based on average score (threshold example, adjust as needed)\n",
        "perf_df[\"performance_band\"] = perf_df[\"avg_score\"].apply(lambda x: \"High\" if x >= 75 else \"Low\")\n",
        "\n",
        "# --- Encode Features ---\n",
        "cat_cols = [\n",
        "    \"gender\", \"age_band\", \"region\", \"school_board\", \"medium_of_instruction\",\n",
        "    \"highest_education\", \"parental_education\", \"internet_access\",\n",
        "    \"device_type\", \"data_recharge_frequency\", \"disability\", \"engagement_pred\"\n",
        "]\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    perf_df[col] = le.fit_transform(perf_df[col].astype(str))\n",
        "\n",
        "# --- Final Feature Set ---\n",
        "features = cat_cols + [\n",
        "    \"studied_credits\", \"num_of_prev_attempts\", \"avg_score\",\n",
        "    \"total_clicks\", \"num_sessions\", \"num_assessments\",\n",
        "    \"clicks_per_session\", \"score_per_assessment\", \"clicks_per_credit\",\n",
        "    \"early_clicks\", \"late_clicks\",\n",
        "    \"engage_prob_low\", \"engage_prob_moderate\", \"engage_prob_high\"\n",
        "]\n",
        "\n",
        "X = perf_df[features]\n",
        "y = LabelEncoder().fit_transform(perf_df[\"performance_band\"])\n",
        "\n",
        "# --- Train/Test Split ---\n",
        "perf_df[\"id_order\"] = perf_df[\"id_student\"].rank(method=\"first\")\n",
        "split = int(len(perf_df) * 0.7)\n",
        "\n",
        "X_train = X[perf_df[\"id_order\"] <= split]\n",
        "X_test = X[perf_df[\"id_order\"] > split]\n",
        "y_train = y[perf_df[\"id_order\"] <= split]\n",
        "y_test = y[perf_df[\"id_order\"] > split]\n",
        "\n",
        "# --- Class Weights ---\n",
        "cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "cw_dict = dict(zip(np.unique(y_train), cw))\n",
        "\n",
        "# --- Models ---\n",
        "xgb = XGBClassifier(scale_pos_weight=cw_dict.get(0, 1), eval_metric='logloss', use_label_encoder=False)\n",
        "lgbm = LGBMClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "lr = LogisticRegression(max_iter=500)\n",
        "\n",
        "# --- Stacking ---\n",
        "stack = StackingClassifier(\n",
        "    estimators=[('xgb', xgb), ('lgbm', lgbm), ('rf', rf), ('lr', lr)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=True\n",
        ")\n",
        "stack.fit(X_train, y_train)\n",
        "final_preds = stack.predict(X_test)\n",
        "\n",
        "# --- Evaluation ---\n",
        "print(\"\\nFinal Stacked Accuracy:\", accuracy_score(y_test, final_preds))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, final_preds))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, final_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlCroKaWCdeR",
        "outputId": "37789792-9a2c-462c-edd7-bc42a2c1d15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 106ms/step - accuracy: 0.3999 - loss: 1.0395 - val_accuracy: 0.6925 - val_loss: 0.6592 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 79ms/step - accuracy: 0.6988 - loss: 0.6211 - val_accuracy: 0.8925 - val_loss: 0.4268 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step - accuracy: 0.8431 - loss: 0.4174 - val_accuracy: 0.8487 - val_loss: 0.3644 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.8290 - loss: 0.4110 - val_accuracy: 0.8712 - val_loss: 0.3087 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 80ms/step - accuracy: 0.8589 - loss: 0.3441 - val_accuracy: 0.8888 - val_loss: 0.2599 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 113ms/step - accuracy: 0.8742 - loss: 0.3247 - val_accuracy: 0.8600 - val_loss: 0.3644 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 80ms/step - accuracy: 0.8666 - loss: 0.3449 - val_accuracy: 0.9000 - val_loss: 0.2535 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.8912 - loss: 0.2867 - val_accuracy: 0.9125 - val_loss: 0.2317 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 86ms/step - accuracy: 0.9012 - loss: 0.2598 - val_accuracy: 0.8438 - val_loss: 0.3842 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 102ms/step - accuracy: 0.8902 - loss: 0.2690 - val_accuracy: 0.9025 - val_loss: 0.2035 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 116ms/step - accuracy: 0.8998 - loss: 0.2400 - val_accuracy: 0.9362 - val_loss: 0.1506 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 81ms/step - accuracy: 0.9207 - loss: 0.1972 - val_accuracy: 0.9350 - val_loss: 0.1545 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 114ms/step - accuracy: 0.9309 - loss: 0.1750 - val_accuracy: 0.9438 - val_loss: 0.1294 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 93ms/step - accuracy: 0.9104 - loss: 0.2065 - val_accuracy: 0.8975 - val_loss: 0.2070 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 112ms/step - accuracy: 0.9052 - loss: 0.2247 - val_accuracy: 0.9125 - val_loss: 0.2149 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 87ms/step - accuracy: 0.9211 - loss: 0.2005 - val_accuracy: 0.9262 - val_loss: 0.1573 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 95ms/step - accuracy: 0.9270 - loss: 0.1639 - val_accuracy: 0.9513 - val_loss: 0.1206 - learning_rate: 1.0000e-04\n",
            "Epoch 18/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.9462 - loss: 0.1349 - val_accuracy: 0.9450 - val_loss: 0.1216 - learning_rate: 1.0000e-04\n",
            "Epoch 19/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 117ms/step - accuracy: 0.9393 - loss: 0.1409 - val_accuracy: 0.9525 - val_loss: 0.1150 - learning_rate: 1.0000e-04\n",
            "Epoch 20/20\n",
            "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 79ms/step - accuracy: 0.9379 - loss: 0.1423 - val_accuracy: 0.9538 - val_loss: 0.1106 - learning_rate: 1.0000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:06:18] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 3055, number of negative: 445\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000725 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2689\n",
            "[LightGBM] [Info] Number of data points in the train set: 3500, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.872857 -> initscore=1.926461\n",
            "[LightGBM] [Info] Start training from score 1.926461\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:06:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:06:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:06:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:06:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:06:19] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 2444, number of negative: 356\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000341 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2689\n",
            "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.872857 -> initscore=1.926461\n",
            "[LightGBM] [Info] Start training from score 1.926461\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2444, number of negative: 356\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000316 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2688\n",
            "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.872857 -> initscore=1.926461\n",
            "[LightGBM] [Info] Start training from score 1.926461\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2444, number of negative: 356\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000301 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2687\n",
            "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.872857 -> initscore=1.926461\n",
            "[LightGBM] [Info] Start training from score 1.926461\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2444, number of negative: 356\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2689\n",
            "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.872857 -> initscore=1.926461\n",
            "[LightGBM] [Info] Start training from score 1.926461\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Info] Number of positive: 2444, number of negative: 356\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2689\n",
            "[LightGBM] [Info] Number of data points in the train set: 2800, number of used features: 24\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.872857 -> initscore=1.926461\n",
            "[LightGBM] [Info] Start training from score 1.926461\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Stacked Accuracy: 0.9993333333333333\n",
            "\n",
            "Confusion Matrix:\n",
            " [[ 207    1]\n",
            " [   0 1292]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       208\n",
            "           1       1.00      1.00      1.00      1292\n",
            "\n",
            "    accuracy                           1.00      1500\n",
            "   macro avg       1.00      1.00      1.00      1500\n",
            "weighted avg       1.00      1.00      1.00      1500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace these with your actual accuracies\n",
        "oulad_accuracy = 0.9110   # Example: 83.73%\n",
        "indian_accuracy = 0.9993  # Example: 92.54%\n",
        "\n",
        "# Labels and values\n",
        "datasets = ['OULAD', 'Indian']\n",
        "accuracies = [oulad_accuracy * 100, indian_accuracy * 100]  # Convert to percentage\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "bars = plt.bar(datasets, accuracies, color=['skyblue', 'orange'])\n",
        "\n",
        "# Annotate bars\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval - 6, f'{yval:.2f}%', ha='center', va='bottom', fontsize=12, color='white', fontweight='bold')\n",
        "\n",
        "# Main chart settings\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.ylim(0, 100)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "# Add heading below the plot\n",
        "plt.figtext(0.5, -0.1, 'Model Accuracy Comparison: OULAD vs Indian Dataset',\n",
        "            ha='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mppMLvGNUiCv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "c77ab303-5daf-4f5f-c0c8-ed2ddd71ac92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZAk_lPBngy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}